{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f68d78e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from sklearn import metrics\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics.cluster import contingency_matrix, homogeneity_score, completeness_score, adjusted_rand_score, adjusted_mutual_info_score, v_measure_score\n",
    "from sklearn.metrics.cluster import contingency_matrix\n",
    "\n",
    "## Import libraries\n",
    "import re\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords' )\n",
    "\n",
    "import string\n",
    "from string import punctuation\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "716a90ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, json, datetime, pytz\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction import text\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from textblob import TextBlob\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from collections import Counter\n",
    "from collections import Counter\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2849c0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytz\n",
    "import datetime\n",
    "\n",
    "pst_tz = pytz.timezone('America/Los_Angeles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "695aaf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet(tweet):\n",
    "    '''\n",
    "    Utility function to clean the text in a tweet by removing \n",
    "    links and special characters using regex.\n",
    "    '''\n",
    "    return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?|\", \" \", tweet).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "72def57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(text, annotate=0):\n",
    "    with open(text+'.txt', 'r',encoding=\"utf8\") as f:\n",
    "        retweet_count, followers, place, title, time, fan = [],[],[],[],[],[]\n",
    "        sentiment_polarity, sentiment_score = [], []\n",
    "        pst_tz = pytz.timezone('US/Pacific') \n",
    "        for line in f:\n",
    "            tweet = json.loads(line)\n",
    "            time_now = datetime.datetime.fromtimestamp(tweet['citation_date'])\n",
    "            retweet_now = tweet['metrics']['citations']['total']\n",
    "            if tweet['tweet']['lang'] == 'en':\n",
    "                title.append(clean_tweet(tweet['title']).lower())\n",
    "                retweet_count.append(tweet['metrics']['citations']['total']) \n",
    "                followers.append(tweet['author']['followers'])\n",
    "                time.append(datetime.datetime.fromtimestamp(tweet['citation_date'], pst_tz))\n",
    "                place.append(tweet['tweet']['user']['location'])\n",
    "                fan.append(annotate)\n",
    "\n",
    "    d = {'title':title, 'retweet_count':retweet_count, 'followers':followers,\n",
    "         'time':time,'place':place, 'fan_base':fan}\n",
    "    df = pd.DataFrame(d)\n",
    "    return df   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "314f6840",
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = open('tweets_#gopatriots.txt', 'r')\n",
    "lines = file1.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7e7d6136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for line in lines:\n",
    "    json_object = json.loads(line)\n",
    "    if json_object['metrics']['citations']['total'] >= 10:\n",
    "        count = count + 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5d3f93df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_patriots = get_data('tweets_#gopatriots').append(get_data('tweets_#patriots'))\n",
    "df_patriots = get_data('tweets_#gopatriots')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d2bdd540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>followers</th>\n",
       "      <th>time</th>\n",
       "      <th>place</th>\n",
       "      <th>fan_base</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LeGarrette Blount does the Ray Lewis Dance Thr...</td>\n",
       "      <td>6</td>\n",
       "      <td>2895.0</td>\n",
       "      <td>2015-01-14 09:45:41-08:00</td>\n",
       "      <td>Boston, Massachusetts</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>With both my father and husband yelling at the...</td>\n",
       "      <td>3</td>\n",
       "      <td>20896.0</td>\n",
       "      <td>2015-01-15 09:11:15-08:00</td>\n",
       "      <td>new york city.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The girls soccer team is ranked the No 3 team ...</td>\n",
       "      <td>3</td>\n",
       "      <td>500.0</td>\n",
       "      <td>2015-01-14 17:26:29-08:00</td>\n",
       "      <td>Plantation, FL</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Patriots Champions RT if you agree GOPATRIOTS</td>\n",
       "      <td>2</td>\n",
       "      <td>491.0</td>\n",
       "      <td>2015-01-16 08:55:32-08:00</td>\n",
       "      <td>Dol Guldur.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12 0 The girls varsity soccer team dominates t...</td>\n",
       "      <td>6</td>\n",
       "      <td>500.0</td>\n",
       "      <td>2015-01-14 17:26:19-08:00</td>\n",
       "      <td>Plantation, FL</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>thanks for the follow GoPatriots</td>\n",
       "      <td>2</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2015-01-14 05:17:19-08:00</td>\n",
       "      <td>Boston, Massachusetts</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ria84 which team do you follow who do you want...</td>\n",
       "      <td>4</td>\n",
       "      <td>10955.0</td>\n",
       "      <td>2015-01-14 01:50:11-08:00</td>\n",
       "      <td>Adelaide, South Australia</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Reminder Old HomeBase Bell Schedule today tomo...</td>\n",
       "      <td>3</td>\n",
       "      <td>1762.0</td>\n",
       "      <td>2015-01-14 02:38:39-08:00</td>\n",
       "      <td>Powell, Ohio</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Can t wait to watch the Super Bowl of this sea...</td>\n",
       "      <td>1</td>\n",
       "      <td>600.0</td>\n",
       "      <td>2015-01-14 04:50:53-08:00</td>\n",
       "      <td>Liverpool, ah no, Chitr√© D:</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>DESIGNS Bx Thanks Great day to you too brotha ...</td>\n",
       "      <td>2</td>\n",
       "      <td>1425.0</td>\n",
       "      <td>2015-01-14 06:21:57-08:00</td>\n",
       "      <td>Kawasaki,Kanagawa,Japan</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>You too Only 4 more days until Gameday GoPatriots</td>\n",
       "      <td>1</td>\n",
       "      <td>1101.0</td>\n",
       "      <td>2015-01-14 07:23:00-08:00</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Beautiful man beautiful numbers GoPatriots Bra...</td>\n",
       "      <td>1</td>\n",
       "      <td>712.0</td>\n",
       "      <td>2015-01-14 08:26:08-08:00</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>10 11 division titles 9 10 AFCchampionship gam...</td>\n",
       "      <td>1</td>\n",
       "      <td>225.0</td>\n",
       "      <td>2015-01-14 10:06:42-08:00</td>\n",
       "      <td>The Universe</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>next time he comes to Gillette he can kiss all...</td>\n",
       "      <td>1</td>\n",
       "      <td>1410.0</td>\n",
       "      <td>2015-01-14 10:11:08-08:00</td>\n",
       "      <td>North of Boston</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>thank you for the follow back Check out our bl...</td>\n",
       "      <td>1</td>\n",
       "      <td>132.0</td>\n",
       "      <td>2015-01-14 10:47:08-08:00</td>\n",
       "      <td>Boston, MA</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Talk to me about this gomason gopatriots fresh...</td>\n",
       "      <td>1</td>\n",
       "      <td>41.0</td>\n",
       "      <td>2015-01-14 11:53:12-08:00</td>\n",
       "      <td>Orlando, FL</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Now departing from Logan Boston ima miss Ya Go...</td>\n",
       "      <td>1</td>\n",
       "      <td>81.0</td>\n",
       "      <td>2015-01-14 11:53:24-08:00</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>my team is getting trough the AFC championship...</td>\n",
       "      <td>1</td>\n",
       "      <td>233.0</td>\n",
       "      <td>2015-01-14 12:25:51-08:00</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>skulls thanks for the follow GoPatriots</td>\n",
       "      <td>2</td>\n",
       "      <td>405.0</td>\n",
       "      <td>2015-01-14 12:36:29-08:00</td>\n",
       "      <td>Longview, Tx</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Double OT win So upset I missed it but so prou...</td>\n",
       "      <td>2</td>\n",
       "      <td>548.0</td>\n",
       "      <td>2015-01-14 18:21:20-08:00</td>\n",
       "      <td>Hurricane, West Virginia</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Shoutout to the bae for the Tom Brady fathead ...</td>\n",
       "      <td>3</td>\n",
       "      <td>5111.0</td>\n",
       "      <td>2015-01-14 20:04:18-08:00</td>\n",
       "      <td>Driver, Seat</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>IDC who wins the NFC championship I just want ...</td>\n",
       "      <td>1</td>\n",
       "      <td>74.0</td>\n",
       "      <td>2015-01-14 20:41:09-08:00</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Chris You MUST PULL 4 our All your girls here ...</td>\n",
       "      <td>4</td>\n",
       "      <td>1357.0</td>\n",
       "      <td>2015-01-15 00:45:06-08:00</td>\n",
       "      <td>New York / Patriots Nation</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>If I don t get to see any of the Patriots vs C...</td>\n",
       "      <td>1</td>\n",
       "      <td>466.0</td>\n",
       "      <td>2015-01-15 01:38:51-08:00</td>\n",
       "      <td>540</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Reminder Old HomeBase Bell Schedule again toda...</td>\n",
       "      <td>1</td>\n",
       "      <td>1762.0</td>\n",
       "      <td>2015-01-15 02:42:11-08:00</td>\n",
       "      <td>Powell, Ohio</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Can t wait for the game this Sunday Brady I ne...</td>\n",
       "      <td>1</td>\n",
       "      <td>29.0</td>\n",
       "      <td>2015-01-15 03:12:00-08:00</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>thanks for the follow GoPatriots GoPatriotsNation</td>\n",
       "      <td>1</td>\n",
       "      <td>3811.0</td>\n",
       "      <td>2015-01-15 05:05:28-08:00</td>\n",
       "      <td>Boston, Massachusetts</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>congrats Dale well deserved classact BigFan go...</td>\n",
       "      <td>1</td>\n",
       "      <td>168.0</td>\n",
       "      <td>2015-01-15 05:24:04-08:00</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>I like hot snickerdoodle coffee to fuel my gam...</td>\n",
       "      <td>1</td>\n",
       "      <td>265.0</td>\n",
       "      <td>2015-01-15 06:05:37-08:00</td>\n",
       "      <td>Boston, MA</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Really hoping that I ll be at the hotel by gam...</td>\n",
       "      <td>1</td>\n",
       "      <td>338.0</td>\n",
       "      <td>2015-01-15 07:10:36-08:00</td>\n",
       "      <td>North Carolina</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Yessir gopatriots</td>\n",
       "      <td>1</td>\n",
       "      <td>622.0</td>\n",
       "      <td>2015-01-15 07:40:54-08:00</td>\n",
       "      <td>N. Ky./Cincy</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Did you see this Thank you for making my son s...</td>\n",
       "      <td>1</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2015-01-15 07:59:50-08:00</td>\n",
       "      <td>Virginia Beach</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>screw you Lewis and everyone else that keeps p...</td>\n",
       "      <td>3</td>\n",
       "      <td>3811.0</td>\n",
       "      <td>2015-01-15 08:24:19-08:00</td>\n",
       "      <td>Boston, Massachusetts</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>thanks for the follow Let s go Patriots GoPatr...</td>\n",
       "      <td>1</td>\n",
       "      <td>3811.0</td>\n",
       "      <td>2015-01-15 08:30:26-08:00</td>\n",
       "      <td>Boston, Massachusetts</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Thank you GoPatriots</td>\n",
       "      <td>1</td>\n",
       "      <td>135.0</td>\n",
       "      <td>2015-01-15 10:39:56-08:00</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Patriots head to Coffman tonight to take on UA...</td>\n",
       "      <td>3</td>\n",
       "      <td>560.0</td>\n",
       "      <td>2015-01-15 10:48:08-08:00</td>\n",
       "      <td>Powell, Ohio</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>thanks for the follow Let s go Patriots GoPatr...</td>\n",
       "      <td>1</td>\n",
       "      <td>3811.0</td>\n",
       "      <td>2015-01-15 11:49:54-08:00</td>\n",
       "      <td>Boston, Massachusetts</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Hi Patricia Thanks so much for your follow too...</td>\n",
       "      <td>3</td>\n",
       "      <td>1357.0</td>\n",
       "      <td>2015-01-15 11:50:00-08:00</td>\n",
       "      <td>New York / Patriots Nation</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>I run on gopatriots</td>\n",
       "      <td>1</td>\n",
       "      <td>1025.0</td>\n",
       "      <td>2015-01-15 12:14:19-08:00</td>\n",
       "      <td>Boston</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>H who you picking this weekend NFLPlayoffs may...</td>\n",
       "      <td>1</td>\n",
       "      <td>50.0</td>\n",
       "      <td>2015-01-15 12:26:35-08:00</td>\n",
       "      <td>Ottawa, Canada</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  retweet_count  \\\n",
       "0   LeGarrette Blount does the Ray Lewis Dance Thr...              6   \n",
       "1   With both my father and husband yelling at the...              3   \n",
       "2   The girls soccer team is ranked the No 3 team ...              3   \n",
       "3       Patriots Champions RT if you agree GOPATRIOTS              2   \n",
       "4   12 0 The girls varsity soccer team dominates t...              6   \n",
       "5                    thanks for the follow GoPatriots              2   \n",
       "6   ria84 which team do you follow who do you want...              4   \n",
       "7   Reminder Old HomeBase Bell Schedule today tomo...              3   \n",
       "8   Can t wait to watch the Super Bowl of this sea...              1   \n",
       "9   DESIGNS Bx Thanks Great day to you too brotha ...              2   \n",
       "10  You too Only 4 more days until Gameday GoPatriots              1   \n",
       "11  Beautiful man beautiful numbers GoPatriots Bra...              1   \n",
       "12  10 11 division titles 9 10 AFCchampionship gam...              1   \n",
       "13  next time he comes to Gillette he can kiss all...              1   \n",
       "14  thank you for the follow back Check out our bl...              1   \n",
       "15  Talk to me about this gomason gopatriots fresh...              1   \n",
       "16  Now departing from Logan Boston ima miss Ya Go...              1   \n",
       "17  my team is getting trough the AFC championship...              1   \n",
       "18            skulls thanks for the follow GoPatriots              2   \n",
       "19  Double OT win So upset I missed it but so prou...              2   \n",
       "20  Shoutout to the bae for the Tom Brady fathead ...              3   \n",
       "21  IDC who wins the NFC championship I just want ...              1   \n",
       "22  Chris You MUST PULL 4 our All your girls here ...              4   \n",
       "23  If I don t get to see any of the Patriots vs C...              1   \n",
       "24  Reminder Old HomeBase Bell Schedule again toda...              1   \n",
       "25  Can t wait for the game this Sunday Brady I ne...              1   \n",
       "26  thanks for the follow GoPatriots GoPatriotsNation              1   \n",
       "27  congrats Dale well deserved classact BigFan go...              1   \n",
       "28  I like hot snickerdoodle coffee to fuel my gam...              1   \n",
       "29  Really hoping that I ll be at the hotel by gam...              1   \n",
       "30                                  Yessir gopatriots              1   \n",
       "31  Did you see this Thank you for making my son s...              1   \n",
       "32  screw you Lewis and everyone else that keeps p...              3   \n",
       "33  thanks for the follow Let s go Patriots GoPatr...              1   \n",
       "34                               Thank you GoPatriots              1   \n",
       "35  Patriots head to Coffman tonight to take on UA...              3   \n",
       "36  thanks for the follow Let s go Patriots GoPatr...              1   \n",
       "37  Hi Patricia Thanks so much for your follow too...              3   \n",
       "38                                I run on gopatriots              1   \n",
       "39  H who you picking this weekend NFLPlayoffs may...              1   \n",
       "\n",
       "    followers                      time                         place  \\\n",
       "0      2895.0 2015-01-14 09:45:41-08:00         Boston, Massachusetts   \n",
       "1     20896.0 2015-01-15 09:11:15-08:00                new york city.   \n",
       "2       500.0 2015-01-14 17:26:29-08:00                Plantation, FL   \n",
       "3       491.0 2015-01-16 08:55:32-08:00                   Dol Guldur.   \n",
       "4       500.0 2015-01-14 17:26:19-08:00                Plantation, FL   \n",
       "5        34.0 2015-01-14 05:17:19-08:00         Boston, Massachusetts   \n",
       "6     10955.0 2015-01-14 01:50:11-08:00     Adelaide, South Australia   \n",
       "7      1762.0 2015-01-14 02:38:39-08:00                  Powell, Ohio   \n",
       "8       600.0 2015-01-14 04:50:53-08:00  Liverpool, ah no, Chitr√© D:    \n",
       "9      1425.0 2015-01-14 06:21:57-08:00       Kawasaki,Kanagawa,Japan   \n",
       "10     1101.0 2015-01-14 07:23:00-08:00                 Massachusetts   \n",
       "11      712.0 2015-01-14 08:26:08-08:00                                 \n",
       "12      225.0 2015-01-14 10:06:42-08:00                 The Universe    \n",
       "13     1410.0 2015-01-14 10:11:08-08:00               North of Boston   \n",
       "14      132.0 2015-01-14 10:47:08-08:00                    Boston, MA   \n",
       "15       41.0 2015-01-14 11:53:12-08:00                   Orlando, FL   \n",
       "16       81.0 2015-01-14 11:53:24-08:00                                 \n",
       "17      233.0 2015-01-14 12:25:51-08:00                                 \n",
       "18      405.0 2015-01-14 12:36:29-08:00                  Longview, Tx   \n",
       "19      548.0 2015-01-14 18:21:20-08:00      Hurricane, West Virginia   \n",
       "20     5111.0 2015-01-14 20:04:18-08:00                 Driver, Seat    \n",
       "21       74.0 2015-01-14 20:41:09-08:00                                 \n",
       "22     1357.0 2015-01-15 00:45:06-08:00   New York / Patriots Nation    \n",
       "23      466.0 2015-01-15 01:38:51-08:00                           540   \n",
       "24     1762.0 2015-01-15 02:42:11-08:00                  Powell, Ohio   \n",
       "25       29.0 2015-01-15 03:12:00-08:00                                 \n",
       "26     3811.0 2015-01-15 05:05:28-08:00         Boston, Massachusetts   \n",
       "27      168.0 2015-01-15 05:24:04-08:00                                 \n",
       "28      265.0 2015-01-15 06:05:37-08:00                    Boston, MA   \n",
       "29      338.0 2015-01-15 07:10:36-08:00               North Carolina    \n",
       "30      622.0 2015-01-15 07:40:54-08:00                  N. Ky./Cincy   \n",
       "31       55.0 2015-01-15 07:59:50-08:00                Virginia Beach   \n",
       "32     3811.0 2015-01-15 08:24:19-08:00         Boston, Massachusetts   \n",
       "33     3811.0 2015-01-15 08:30:26-08:00         Boston, Massachusetts   \n",
       "34      135.0 2015-01-15 10:39:56-08:00                                 \n",
       "35      560.0 2015-01-15 10:48:08-08:00                  Powell, Ohio   \n",
       "36     3811.0 2015-01-15 11:49:54-08:00         Boston, Massachusetts   \n",
       "37     1357.0 2015-01-15 11:50:00-08:00   New York / Patriots Nation    \n",
       "38     1025.0 2015-01-15 12:14:19-08:00                       Boston    \n",
       "39       50.0 2015-01-15 12:26:35-08:00                Ottawa, Canada   \n",
       "\n",
       "    fan_base  \n",
       "0          0  \n",
       "1          0  \n",
       "2          0  \n",
       "3          0  \n",
       "4          0  \n",
       "5          0  \n",
       "6          0  \n",
       "7          0  \n",
       "8          0  \n",
       "9          0  \n",
       "10         0  \n",
       "11         0  \n",
       "12         0  \n",
       "13         0  \n",
       "14         0  \n",
       "15         0  \n",
       "16         0  \n",
       "17         0  \n",
       "18         0  \n",
       "19         0  \n",
       "20         0  \n",
       "21         0  \n",
       "22         0  \n",
       "23         0  \n",
       "24         0  \n",
       "25         0  \n",
       "26         0  \n",
       "27         0  \n",
       "28         0  \n",
       "29         0  \n",
       "30         0  \n",
       "31         0  \n",
       "32         0  \n",
       "33         0  \n",
       "34         0  \n",
       "35         0  \n",
       "36         0  \n",
       "37         0  \n",
       "38         0  \n",
       "39         0  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_patriots.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "31c5e6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hawks = get_data('tweets_#gohawks', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0f825fe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>followers</th>\n",
       "      <th>time</th>\n",
       "      <th>place</th>\n",
       "      <th>fan_base</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I lt 3 our defense GoHawks</td>\n",
       "      <td>5</td>\n",
       "      <td>1752.0</td>\n",
       "      <td>2015-01-17 10:19:38-08:00</td>\n",
       "      <td>Ontario</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>twelfth dogs are ready gohawks dogslife</td>\n",
       "      <td>2</td>\n",
       "      <td>258.0</td>\n",
       "      <td>2015-01-14 10:18:56-08:00</td>\n",
       "      <td>Redmond, WA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Oh no big deal just NFC West Champs and the nu...</td>\n",
       "      <td>5</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2015-01-16 20:21:59-08:00</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Good luck at Michigan Jim Harbaugh GoHawks</td>\n",
       "      <td>2</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2015-01-16 20:18:56-08:00</td>\n",
       "      <td>Buffalo, Wyoming</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GoHawks Keep your eyes on the ball Peyton</td>\n",
       "      <td>2</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2015-01-16 20:16:16-08:00</td>\n",
       "      <td>McChord AFB, WA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>One of my Favorite Hawk logos that went around...</td>\n",
       "      <td>2</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2015-01-16 20:15:44-08:00</td>\n",
       "      <td>Washington state</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HAPPY NEW YEAR EVERYONE Here s to a 2015 full ...</td>\n",
       "      <td>3</td>\n",
       "      <td>186.0</td>\n",
       "      <td>2015-01-16 00:08:22-08:00</td>\n",
       "      <td>#Seahawks #Mariners</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>My hometown GoHawks pics Seattle Washington USA</td>\n",
       "      <td>2</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2015-01-16 20:13:01-08:00</td>\n",
       "      <td>Pacific Northwest - USA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Absolutely GoHawks</td>\n",
       "      <td>8</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2015-01-16 20:12:16-08:00</td>\n",
       "      <td>The Halls of Valhalla</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>12 s are the champions 24 7 TGIBF WhyNotUsAgai...</td>\n",
       "      <td>5</td>\n",
       "      <td>977.0</td>\n",
       "      <td>2015-01-16 18:32:08-08:00</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>If they pass on you pass on them GoHawks</td>\n",
       "      <td>2</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2015-01-16 20:11:25-08:00</td>\n",
       "      <td>Greater Seattle Area</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>At Serve me up some Hawks Seahawks worldchampi...</td>\n",
       "      <td>2</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2015-01-16 20:10:31-08:00</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>My Tee shirt came today I love it 12thMan GoHawks</td>\n",
       "      <td>3</td>\n",
       "      <td>1752.0</td>\n",
       "      <td>2015-01-16 08:26:48-08:00</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Wise Marshawn Lynch life mottos Seahawks GoHaw...</td>\n",
       "      <td>2</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2015-01-16 20:09:11-08:00</td>\n",
       "      <td>Washington</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Signing out bedtime GoHawks repeat</td>\n",
       "      <td>2</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2015-01-16 20:07:55-08:00</td>\n",
       "      <td>Bout that action Boss, SD</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1st tweet Huge thanks to Coach Ferentz amp Iow...</td>\n",
       "      <td>81</td>\n",
       "      <td>150.0</td>\n",
       "      <td>2015-01-17 13:51:06-08:00</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>I ll say 2015 is gonna be a beautiful year ahe...</td>\n",
       "      <td>3</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2015-01-16 20:07:14-08:00</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>How are you expressing your 12th Manliness goh...</td>\n",
       "      <td>3</td>\n",
       "      <td>3883.0</td>\n",
       "      <td>2015-01-16 15:59:19-08:00</td>\n",
       "      <td>Bellevue, WA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>You been 12ed pass it on SeahawkNation LOB Sup...</td>\n",
       "      <td>15</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2015-01-16 20:06:19-08:00</td>\n",
       "      <td>k-town Ak.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Now we can add Ben to this GOHAWKS</td>\n",
       "      <td>9</td>\n",
       "      <td>62.0</td>\n",
       "      <td>2015-01-14 14:23:56-08:00</td>\n",
       "      <td>Las Vegas</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Oh my 12s 12thMan SeaHawks Seattle Seahawks Go...</td>\n",
       "      <td>6</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2015-01-16 20:00:41-08:00</td>\n",
       "      <td>N.W. Washington State, USA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Be ready for another BeastQuake on Saturday CA...</td>\n",
       "      <td>2</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2015-01-16 20:00:23-08:00</td>\n",
       "      <td>Washington #PNW</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Sonics fans are in ImIn NFLPlayoffs GoHawks Br...</td>\n",
       "      <td>57</td>\n",
       "      <td>381.0</td>\n",
       "      <td>2015-01-17 12:29:14-08:00</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>No better place on the map to be No better tea...</td>\n",
       "      <td>7</td>\n",
       "      <td>1752.0</td>\n",
       "      <td>2015-01-15 16:53:24-08:00</td>\n",
       "      <td>Shelton, WA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Fine Hawkitecture art RT SAM is IN ImIn Hammer...</td>\n",
       "      <td>80</td>\n",
       "      <td>5938.0</td>\n",
       "      <td>2015-01-14 15:09:48-08:00</td>\n",
       "      <td>Seattle, Washington</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Just because he was born n Tennessee doesn t m...</td>\n",
       "      <td>4</td>\n",
       "      <td>186.0</td>\n",
       "      <td>2015-01-16 00:08:04-08:00</td>\n",
       "      <td>#Seahawks #Mariners</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>I am too freaking excited for Saturday Seahawk...</td>\n",
       "      <td>3</td>\n",
       "      <td>5938.0</td>\n",
       "      <td>2015-01-14 15:08:52-08:00</td>\n",
       "      <td>Edmonds, WA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>I m all the way on the field in ImIn HawkNatio...</td>\n",
       "      <td>3</td>\n",
       "      <td>5937.0</td>\n",
       "      <td>2015-01-14 15:08:32-08:00</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>The Road to SuperBowl goes through Seattle GoH...</td>\n",
       "      <td>14</td>\n",
       "      <td>569.0</td>\n",
       "      <td>2015-01-14 01:21:33-08:00</td>\n",
       "      <td>Seattle, Wa</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Hey We Are Seattle and We Are In ProtectOurHom...</td>\n",
       "      <td>2</td>\n",
       "      <td>317.0</td>\n",
       "      <td>2015-01-15 14:29:41-08:00</td>\n",
       "      <td>Greater Seattle</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>The pine cone curtain is in RT No doubt about ...</td>\n",
       "      <td>81</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2015-01-14 18:44:31-08:00</td>\n",
       "      <td>Seattle, Washington</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>I know we ve moved on to bigger better things ...</td>\n",
       "      <td>2</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2015-01-16 19:54:14-08:00</td>\n",
       "      <td>Spokane, WA, USA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Hey 12s it is The Day Before the Day Before Go...</td>\n",
       "      <td>534</td>\n",
       "      <td>5938.0</td>\n",
       "      <td>2015-01-14 22:22:00-08:00</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Support amp KickHungerChallenge w Seahawks tee...</td>\n",
       "      <td>141</td>\n",
       "      <td>10118.0</td>\n",
       "      <td>2015-01-16 11:33:21-08:00</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>How 12 are you These natl guard members are su...</td>\n",
       "      <td>21</td>\n",
       "      <td>36072.0</td>\n",
       "      <td>2015-01-16 09:18:22-08:00</td>\n",
       "      <td>Camp Murray, Washington</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>TBT 9 2013 My oldest son Mikael drew a pic of ...</td>\n",
       "      <td>2</td>\n",
       "      <td>733.0</td>\n",
       "      <td>2015-01-16 18:33:29-08:00</td>\n",
       "      <td>Monroe, Washington</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>The night before the night before GoHawks</td>\n",
       "      <td>702</td>\n",
       "      <td>65.0</td>\n",
       "      <td>2015-01-14 22:36:45-08:00</td>\n",
       "      <td>Safeco Field</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>I LOVE this and it will be my next Sweatshirt ...</td>\n",
       "      <td>2</td>\n",
       "      <td>141.0</td>\n",
       "      <td>2015-01-16 14:38:32-08:00</td>\n",
       "      <td>Seattle Washington</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Reppin for my Seahawks NFLPlayoffs GoHawks Fri...</td>\n",
       "      <td>21</td>\n",
       "      <td>800.0</td>\n",
       "      <td>2015-01-15 17:24:52-08:00</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Happy Bluefriday 12thman GoHawks beastmode twe...</td>\n",
       "      <td>9</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2015-01-17 10:14:04-08:00</td>\n",
       "      <td>Seattle, Washington</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  retweet_count  \\\n",
       "0                          I lt 3 our defense GoHawks              5   \n",
       "1             twelfth dogs are ready gohawks dogslife              2   \n",
       "2   Oh no big deal just NFC West Champs and the nu...              5   \n",
       "3          Good luck at Michigan Jim Harbaugh GoHawks              2   \n",
       "4           GoHawks Keep your eyes on the ball Peyton              2   \n",
       "5   One of my Favorite Hawk logos that went around...              2   \n",
       "6   HAPPY NEW YEAR EVERYONE Here s to a 2015 full ...              3   \n",
       "7     My hometown GoHawks pics Seattle Washington USA              2   \n",
       "8                                  Absolutely GoHawks              8   \n",
       "9   12 s are the champions 24 7 TGIBF WhyNotUsAgai...              5   \n",
       "10           If they pass on you pass on them GoHawks              2   \n",
       "11  At Serve me up some Hawks Seahawks worldchampi...              2   \n",
       "12  My Tee shirt came today I love it 12thMan GoHawks              3   \n",
       "13  Wise Marshawn Lynch life mottos Seahawks GoHaw...              2   \n",
       "14                 Signing out bedtime GoHawks repeat              2   \n",
       "15  1st tweet Huge thanks to Coach Ferentz amp Iow...             81   \n",
       "16  I ll say 2015 is gonna be a beautiful year ahe...              3   \n",
       "17  How are you expressing your 12th Manliness goh...              3   \n",
       "18  You been 12ed pass it on SeahawkNation LOB Sup...             15   \n",
       "19                 Now we can add Ben to this GOHAWKS              9   \n",
       "20  Oh my 12s 12thMan SeaHawks Seattle Seahawks Go...              6   \n",
       "21  Be ready for another BeastQuake on Saturday CA...              2   \n",
       "22  Sonics fans are in ImIn NFLPlayoffs GoHawks Br...             57   \n",
       "23  No better place on the map to be No better tea...              7   \n",
       "24  Fine Hawkitecture art RT SAM is IN ImIn Hammer...             80   \n",
       "25  Just because he was born n Tennessee doesn t m...              4   \n",
       "26  I am too freaking excited for Saturday Seahawk...              3   \n",
       "27  I m all the way on the field in ImIn HawkNatio...              3   \n",
       "28  The Road to SuperBowl goes through Seattle GoH...             14   \n",
       "29  Hey We Are Seattle and We Are In ProtectOurHom...              2   \n",
       "30  The pine cone curtain is in RT No doubt about ...             81   \n",
       "31  I know we ve moved on to bigger better things ...              2   \n",
       "32  Hey 12s it is The Day Before the Day Before Go...            534   \n",
       "33  Support amp KickHungerChallenge w Seahawks tee...            141   \n",
       "34  How 12 are you These natl guard members are su...             21   \n",
       "35  TBT 9 2013 My oldest son Mikael drew a pic of ...              2   \n",
       "36          The night before the night before GoHawks            702   \n",
       "37  I LOVE this and it will be my next Sweatshirt ...              2   \n",
       "38  Reppin for my Seahawks NFLPlayoffs GoHawks Fri...             21   \n",
       "39  Happy Bluefriday 12thman GoHawks beastmode twe...              9   \n",
       "\n",
       "    followers                      time                       place  fan_base  \n",
       "0      1752.0 2015-01-17 10:19:38-08:00                     Ontario         1  \n",
       "1       258.0 2015-01-14 10:18:56-08:00                 Redmond, WA         1  \n",
       "2        22.0 2015-01-16 20:21:59-08:00                     Seattle         1  \n",
       "3        22.0 2015-01-16 20:18:56-08:00            Buffalo, Wyoming         1  \n",
       "4        22.0 2015-01-16 20:16:16-08:00             McChord AFB, WA         1  \n",
       "5        22.0 2015-01-16 20:15:44-08:00            Washington state         1  \n",
       "6       186.0 2015-01-16 00:08:22-08:00        #Seahawks #Mariners          1  \n",
       "7        22.0 2015-01-16 20:13:01-08:00     Pacific Northwest - USA         1  \n",
       "8        22.0 2015-01-16 20:12:16-08:00       The Halls of Valhalla         1  \n",
       "9       977.0 2015-01-16 18:32:08-08:00                                     1  \n",
       "10       22.0 2015-01-16 20:11:25-08:00        Greater Seattle Area         1  \n",
       "11       22.0 2015-01-16 20:10:31-08:00                                     1  \n",
       "12     1752.0 2015-01-16 08:26:48-08:00                                     1  \n",
       "13       22.0 2015-01-16 20:09:11-08:00                  Washington         1  \n",
       "14       22.0 2015-01-16 20:07:55-08:00   Bout that action Boss, SD         1  \n",
       "15      150.0 2015-01-17 13:51:06-08:00                                     1  \n",
       "16       22.0 2015-01-16 20:07:14-08:00                                     1  \n",
       "17     3883.0 2015-01-16 15:59:19-08:00                Bellevue, WA         1  \n",
       "18       22.0 2015-01-16 20:06:19-08:00                  k-town Ak.         1  \n",
       "19       62.0 2015-01-14 14:23:56-08:00                   Las Vegas         1  \n",
       "20       22.0 2015-01-16 20:00:41-08:00  N.W. Washington State, USA         1  \n",
       "21       22.0 2015-01-16 20:00:23-08:00             Washington #PNW         1  \n",
       "22      381.0 2015-01-17 12:29:14-08:00                 Seattle, WA         1  \n",
       "23     1752.0 2015-01-15 16:53:24-08:00                 Shelton, WA         1  \n",
       "24     5938.0 2015-01-14 15:09:48-08:00         Seattle, Washington         1  \n",
       "25      186.0 2015-01-16 00:08:04-08:00        #Seahawks #Mariners          1  \n",
       "26     5938.0 2015-01-14 15:08:52-08:00                 Edmonds, WA         1  \n",
       "27     5937.0 2015-01-14 15:08:32-08:00                                     1  \n",
       "28      569.0 2015-01-14 01:21:33-08:00                 Seattle, Wa         1  \n",
       "29      317.0 2015-01-15 14:29:41-08:00             Greater Seattle         1  \n",
       "30        7.0 2015-01-14 18:44:31-08:00         Seattle, Washington         1  \n",
       "31       22.0 2015-01-16 19:54:14-08:00            Spokane, WA, USA         1  \n",
       "32     5938.0 2015-01-14 22:22:00-08:00                     Seattle         1  \n",
       "33    10118.0 2015-01-16 11:33:21-08:00                 Seattle, WA         1  \n",
       "34    36072.0 2015-01-16 09:18:22-08:00     Camp Murray, Washington         1  \n",
       "35      733.0 2015-01-16 18:33:29-08:00         Monroe, Washington          1  \n",
       "36       65.0 2015-01-14 22:36:45-08:00                Safeco Field         1  \n",
       "37      141.0 2015-01-16 14:38:32-08:00          Seattle Washington         1  \n",
       "38      800.0 2015-01-15 17:24:52-08:00                     Seattle         1  \n",
       "39       22.0 2015-01-17 10:14:04-08:00         Seattle, Washington         1  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hawks.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "981a3cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12884\n",
      "142367\n"
     ]
    }
   ],
   "source": [
    "print(len(df_patriots))\n",
    "print(len(df_hawks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "281d5d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import libraries\n",
    "import re\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords' )\n",
    "\n",
    "import string\n",
    "from string import punctuation\n",
    "\n",
    "stop_words_skt = text.ENGLISH_STOP_WORDS\n",
    "stop_words_en = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7592ed50",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_game = ['patriots', 'hawks', 'gopatriots', 'gohawks', 'patriot', 'hawk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1efac0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_stopwords = set.union(set(stop_words_en),set(punctuation),set(stop_words_skt), set(stop_words_game))\n",
    "wnl = nltk.wordnet.WordNetLemmatizer()\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "analyzer = CountVectorizer().build_analyzer()\n",
    "\n",
    "def penn2morphy(penntag):\n",
    "    \"\"\" \n",
    "    Converts Penn Treebank tags to WordNet. \n",
    "    \"\"\"\n",
    "    morphy_tag = {'NN':'n', 'JJ':'a',\n",
    "                  'VB':'v', 'RB':'r'}\n",
    "    try:\n",
    "        return morphy_tag[penntag[:2]]\n",
    "    except:\n",
    "        return 'n' \n",
    "\n",
    "def lemmatize_sent(list_word): \n",
    "    '''\n",
    "    Returns lemmatized set of tokens with pos tagging\n",
    "    '''\n",
    "    return [wnl.lemmatize(word.lower(), pos=penn2morphy(tag)) \n",
    "            for word, tag in pos_tag(list_word)]\n",
    "\n",
    "def stem_rmv_punc(doc): # this should have been at the sentence-level because the pos-tag performs best at sentence-level\n",
    "    return (word for word in lemmatize_sent(analyzer(doc)) if word not in combined_stopwords and not word.isdigit())\n",
    "\n",
    "def clean_tokens_lemma (tokens: list):\n",
    "    '''\n",
    "    Cleans set of tokens at sentence level by applying lemmatization at sentence level\n",
    "    '''\n",
    "    lower_txt = [token.lower() for token in tokens]\n",
    "    remove_words = [token for token in lower_txt if (not token.isdigit())\\\n",
    "                     and (token not in combined_stopwords) and (len(token)>1)]\n",
    "    lemmatize_tokens = lemmatize_sent(remove_words) \n",
    "    return lemmatize_tokens\n",
    "\n",
    "def doc_tokens_lemma (doc):\n",
    "    '''\n",
    "    Split the document at sentence level, clean it and return clean set of tokens at doc level\n",
    "    '''\n",
    "    doc = clean(doc)\n",
    "    list_sentences = sent_tokenize(doc)\n",
    "    doc_tokens = []\n",
    "    for sentence in list_sentences:\n",
    "        sentence = sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        tokens = clean_tokens_lemma(tokens)\n",
    "        doc_tokens.extend(tokens)\n",
    "#     print(doc_tokens)\n",
    "    return (word for word in doc_tokens)     \n",
    "\n",
    "def doc_tokens_lemma_woClean (doc):\n",
    "    '''\n",
    "    Perform lemmatization without any text cleaning\n",
    "    '''\n",
    "    list_sentences = sent_tokenize(doc)\n",
    "    doc_tokens = []\n",
    "    for sentence in list_sentences:\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        tokens = lemmatize_sent(tokens)\n",
    "        doc_tokens.extend(tokens)\n",
    "#     print(doc_tokens)\n",
    "    return (word for word in doc_tokens)   \n",
    "\n",
    "def doc_tokens_stem (doc):\n",
    "    '''\n",
    "    Clean full text using stemming. This function does not require split at sentence level.\n",
    "    '''\n",
    "    doc = clean(doc)\n",
    "    doc = doc.translate(str.maketrans('', '', string.punctuation))\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "    lower_txt = [token.lower() for token in tokens]\n",
    "    remove_words = [token for token in lower_txt if (not token.isdigit())\\\n",
    "                     and (token not in combined_stopwords) and (len(token)>1)]\n",
    "    stem_tokens = [stemmer.stem(token) for token in remove_words] \n",
    "    return (word for word in stem_tokens)\n",
    "\n",
    "def doc_tokens_stem_woClean (doc):\n",
    "    '''\n",
    "    Clean full text using stemming. This function does not require split at sentence level.\n",
    "    '''\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "    lower_txt = [token.lower() for token in tokens]\n",
    "    stem_tokens = [stemmer.stem(token) for token in lower_txt] \n",
    "    return (word for word in stem_tokens)\n",
    "    \n",
    "def doc_tokens (doc):\n",
    "    '''\n",
    "    Clean full text without any stemming or lemmatization\n",
    "    '''\n",
    "    doc = clean(doc)\n",
    "    doc = doc.translate(str.maketrans('', '', string.punctuation))\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "    lower_txt = [token.lower() for token in tokens]\n",
    "    remove_words = [token for token in lower_txt if (not token.isdigit())\\\n",
    "                     and (token not in combined_stopwords) and (len(token)>1)]\n",
    "    return (word for word in remove_words)\n",
    "    \n",
    "def doc_tokens_woClean (doc):\n",
    "    '''\n",
    "    Clean full text using stemming. This function does not require split at sentence level.\n",
    "    '''\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "    lower_txt = [token.lower() for token in tokens]\n",
    "    return (word for word in lower_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6752f1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    '''\n",
    "    Helps remove many HTML artefacts from the crawler's output.\n",
    "    '''\n",
    "    text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    texter = re.sub(r\"<br />\", \" \", text)\n",
    "    texter = re.sub(r\"&quot;\", \"\\\"\",texter)\n",
    "    texter = re.sub('&#39;', \"\\\"\", texter)\n",
    "    texter = re.sub('\\n', \" \", texter)\n",
    "    texter = re.sub(' u ',\" you \", texter)\n",
    "    texter = re.sub('`',\"\", texter)\n",
    "    texter = re.sub(' +', ' ', texter)\n",
    "    texter = re.sub(r\"(!)\\1+\", r\"!\", texter)\n",
    "    texter = re.sub(r\"(\\?)\\1+\", r\"?\", texter)\n",
    "    texter = re.sub('&amp;', 'and', texter)\n",
    "    texter = re.sub('\\r', ' ',texter)\n",
    "    clean = re.compile('<.*?>')\n",
    "    texter = texter.encode('ascii', 'ignore').decode('ascii')\n",
    "    texter = re.sub(clean, '', texter)\n",
    "    if texter == \"\":\n",
    "        texter = \"\"\n",
    "    return texter\n",
    "\n",
    "def clean_data(data):\n",
    "    for index,row in data.iterrows():\n",
    "        row['full_text'] = clean(row['full_text'])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3d0de5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def count_train_test(train,test):\n",
    "    \n",
    "    train_num = train.shape[0]\n",
    "    test_num = test.shape[0]\n",
    "    return train_num,test_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1dfbe024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train number:10307\n",
      "test number:2577\n"
     ]
    }
   ],
   "source": [
    "train_patriots, test_patriots = train_test_split(df_patriots, test_size=0.2, random_state = 42)\n",
    "\n",
    "train_num,test_num = count_train_test(train_patriots,test_patriots)\n",
    "print(\"train number:{}\".format(train_num))\n",
    "print(\"test number:{}\".format(test_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "485e8af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train number:113893\n",
      "test number:28474\n"
     ]
    }
   ],
   "source": [
    "train_hawks, test_hawks = train_test_split(df_hawks, test_size=0.2, random_state = 42)\n",
    "\n",
    "train_num,test_num = count_train_test(train_hawks, test_hawks)\n",
    "print(\"train number:{}\".format(train_num))\n",
    "print(\"test number:{}\".format(test_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3e9106ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF processed train data shape: (10307, 1915)\n",
      "TF-IDF processed test data shape: (2577, 1915)\n"
     ]
    }
   ],
   "source": [
    "## For Patriots\n",
    "\n",
    "count_vectorizer_patriots = CountVectorizer(min_df=3, stop_words='english', analyzer=doc_tokens_lemma, max_df=0.7)\n",
    "transformer = TfidfTransformer()\n",
    "\n",
    "X_train_counts_patriots = count_vectorizer_patriots.fit_transform(train_patriots['title'])\n",
    "X_train_tfidf_patriots = transformer.fit_transform(X_train_counts_patriots)\n",
    "\n",
    "X_test_counts_patriots = count_vectorizer_patriots.transform(test_patriots['title'])\n",
    "X_test_tfidf_patriots = transformer.transform(X_test_counts_patriots)\n",
    "\n",
    "print('TF-IDF processed train data shape:', X_train_tfidf_patriots.shape)\n",
    "print('TF-IDF processed test data shape:', X_test_tfidf_patriots.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fb2a0942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF processed train data shape: (113893, 10374)\n",
      "TF-IDF processed test data shape: (28474, 10374)\n"
     ]
    }
   ],
   "source": [
    "## For Hawks\n",
    "\n",
    "count_vectorizer_hawks = CountVectorizer(min_df=3, stop_words='english', analyzer=doc_tokens_lemma, max_df=0.7)\n",
    "transformer = TfidfTransformer()\n",
    "\n",
    "X_train_counts_hawks = count_vectorizer_hawks.fit_transform(train_hawks['title'])\n",
    "X_train_tfidf_hawks = transformer.fit_transform(X_train_counts_hawks)\n",
    "\n",
    "X_test_counts_hawks = count_vectorizer_hawks.transform(test_hawks['title'])\n",
    "X_test_tfidf_hawks = transformer.transform(X_test_counts_hawks)\n",
    "\n",
    "print('TF-IDF processed train data shape:', X_train_tfidf_hawks.shape)\n",
    "print('TF-IDF processed test data shape:', X_test_tfidf_hawks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "05a4580b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_patriots.append(train_hawks.sample(n=len(train_patriots)))\n",
    "test = test_patriots.append(test_hawks.sample(n=len(train_patriots)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5396d5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop_duplicates()\n",
    "test = test.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0fd6d1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from tempfile import mkdtemp\n",
    "from shutil import rmtree\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "11af0744",
   "metadata": {},
   "outputs": [],
   "source": [
    "cachedir = mkdtemp()\n",
    "memory = joblib.Memory(location=cachedir, verbose=10)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(min_df=3, analyzer=doc_tokens_lemma)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('reduce_dim', TruncatedSVD(random_state=42)),\n",
    "    ('clf', GaussianNB()),\n",
    "],\n",
    "memory=memory\n",
    ")\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        'vect': [CountVectorizer(min_df=3, analyzer=doc_tokens_lemma),\n",
    "        ],\n",
    "        'reduce_dim': [\n",
    "                        TruncatedSVD(n_components=500, random_state=42),\n",
    "#                         NMF(n_components=500, init='random', random_state=42),\n",
    "        ],\n",
    "        'clf': [\n",
    "                SVC(kernel='linear', C=10, random_state=42),\n",
    "                LogisticRegression(penalty='l2', C=10, random_state=42),\n",
    "                GaussianNB(),\n",
    "                RandomForestClassifier(max_depth=10, random_state=42)\n",
    "        ],\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ceb8cb9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "[Memory] Calling sklearn.pipeline._fit_transform_one...\n",
      "_fit_transform_one(CountVectorizer(analyzer=<function doc_tokens_lemma at 0x7fbc689839d0>,\n",
      "                min_df=3), \n",
      "7089      The Seahawks better not win this Super Bowl Go...\n",
      "8473      Yall seahawk fans quiet yall aint making no no...\n",
      "12112                    patriots GoPatriots Finish the Job\n",
      "12181                         SuperBowl reaction GoPatriots\n",
      "8794                      Don t let me down Pats gopatriots\n",
      "                                ...                        \n",
      "70978           How many yall going to the send off GoHawks\n",
      "119004             I hope the Seahawks beat Seattle GoHawks\n",
      "112548              GoHawks watching the from Venezuela Yay\n",
      "22201                    That 25 guy is pretty good GoHawks\n",
      "108966    If you do not let the world know who you re ch...\n",
      "Name: title, Length: 13742, dtype: object, \n",
      "7089      0\n",
      "8473      0\n",
      "12112     0\n",
      "12181     0\n",
      "8794      0\n",
      "         ..\n",
      "70978     1\n",
      "119004    1\n",
      "112548    1\n",
      "22201     1\n",
      "108966    1\n",
      "Name: fan_base, Length: 13742, dtype: int64, \n",
      "None, message_clsname='Pipeline', message=None)\n",
      "________________________________________________fit_transform_one - 5.3s, 0.1min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling sklearn.pipeline._fit_transform_one...\n",
      "_fit_transform_one(TfidfTransformer(), <13742x2594 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 76934 stored elements in Compressed Sparse Row format>, \n",
      "7089      0\n",
      "8473      0\n",
      "12112     0\n",
      "12181     0\n",
      "8794      0\n",
      "         ..\n",
      "70978     1\n",
      "119004    1\n",
      "112548    1\n",
      "22201     1\n",
      "108966    1\n",
      "Name: fan_base, Length: 13742, dtype: int64, \n",
      "None, message_clsname='Pipeline', message=None)\n",
      "________________________________________________fit_transform_one - 0.0s, 0.0min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling sklearn.pipeline._fit_transform_one...\n",
      "_fit_transform_one(TruncatedSVD(n_components=500, random_state=42), <13742x2594 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 76934 stored elements in Compressed Sparse Row format>, \n",
      "7089      0\n",
      "8473      0\n",
      "12112     0\n",
      "12181     0\n",
      "8794      0\n",
      "         ..\n",
      "70978     1\n",
      "119004    1\n",
      "112548    1\n",
      "22201     1\n",
      "108966    1\n",
      "Name: fan_base, Length: 13742, dtype: int64, \n",
      "None, message_clsname='Pipeline', message=None)\n",
      "________________________________________________fit_transform_one - 1.8s, 0.0min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling sklearn.pipeline._fit_transform_one...\n",
      "_fit_transform_one(CountVectorizer(analyzer=<function doc_tokens_lemma at 0x7fbc689839d0>,\n",
      "                min_df=3), \n",
      "6436      Lettttzzzz go Game time boom SuperBowl49 GoPat...\n",
      "1805      Is there a mercy rule in the NFL My god what a...\n",
      "2288      I don t like either team but for sake of compe...\n",
      "11736     Seahawks some sore losers bro tryna fight and ...\n",
      "7047      For the love of God please stop putting a Kard...\n",
      "                                ...                        \n",
      "70978           How many yall going to the send off GoHawks\n",
      "119004             I hope the Seahawks beat Seattle GoHawks\n",
      "112548              GoHawks watching the from Venezuela Yay\n",
      "22201                    That 25 guy is pretty good GoHawks\n",
      "108966    If you do not let the world know who you re ch...\n",
      "Name: title, Length: 13743, dtype: object, \n",
      "6436      0\n",
      "1805      0\n",
      "2288      0\n",
      "11736     0\n",
      "7047      0\n",
      "         ..\n",
      "70978     1\n",
      "119004    1\n",
      "112548    1\n",
      "22201     1\n",
      "108966    1\n",
      "Name: fan_base, Length: 13743, dtype: int64, \n",
      "None, message_clsname='Pipeline', message=None)\n",
      "________________________________________________fit_transform_one - 5.3s, 0.1min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling sklearn.pipeline._fit_transform_one...\n",
      "_fit_transform_one(TfidfTransformer(), <13743x2604 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 77067 stored elements in Compressed Sparse Row format>, \n",
      "6436      0\n",
      "1805      0\n",
      "2288      0\n",
      "11736     0\n",
      "7047      0\n",
      "         ..\n",
      "70978     1\n",
      "119004    1\n",
      "112548    1\n",
      "22201     1\n",
      "108966    1\n",
      "Name: fan_base, Length: 13743, dtype: int64, \n",
      "None, message_clsname='Pipeline', message=None)\n",
      "________________________________________________fit_transform_one - 0.0s, 0.0min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling sklearn.pipeline._fit_transform_one...\n",
      "_fit_transform_one(TruncatedSVD(n_components=500, random_state=42), <13743x2604 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 77067 stored elements in Compressed Sparse Row format>, \n",
      "6436      0\n",
      "1805      0\n",
      "2288      0\n",
      "11736     0\n",
      "7047      0\n",
      "         ..\n",
      "70978     1\n",
      "119004    1\n",
      "112548    1\n",
      "22201     1\n",
      "108966    1\n",
      "Name: fan_base, Length: 13743, dtype: int64, \n",
      "None, message_clsname='Pipeline', message=None)\n",
      "________________________________________________fit_transform_one - 1.7s, 0.0min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling sklearn.pipeline._fit_transform_one...\n",
      "_fit_transform_one(CountVectorizer(analyzer=<function doc_tokens_lemma at 0x7fbc689839d0>,\n",
      "                min_df=3), \n",
      "6436      Lettttzzzz go Game time boom SuperBowl49 GoPat...\n",
      "1805      Is there a mercy rule in the NFL My god what a...\n",
      "2288      I don t like either team but for sake of compe...\n",
      "11736     Seahawks some sore losers bro tryna fight and ...\n",
      "7047      For the love of God please stop putting a Kard...\n",
      "                                ...                        \n",
      "90344     We got that beer game going strong NFL GoHawks...\n",
      "42038     PACKERS ARE KEARSED SeahawksvsPackers Seahawks...\n",
      "101295             We have arrived 12thMan Seahawks GoHawks\n",
      "54444     What of country do you think will root for Sea...\n",
      "56028     Wilson s 4 picks were all intended for Kearse ...\n",
      "Name: title, Length: 13743, dtype: object, \n",
      "6436      0\n",
      "1805      0\n",
      "2288      0\n",
      "11736     0\n",
      "7047      0\n",
      "         ..\n",
      "90344     1\n",
      "42038     1\n",
      "101295    1\n",
      "54444     1\n",
      "56028     1\n",
      "Name: fan_base, Length: 13743, dtype: int64, \n",
      "None, message_clsname='Pipeline', message=None)\n",
      "________________________________________________fit_transform_one - 5.3s, 0.1min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling sklearn.pipeline._fit_transform_one...\n",
      "_fit_transform_one(TfidfTransformer(), <13743x2611 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 77247 stored elements in Compressed Sparse Row format>, \n",
      "6436      0\n",
      "1805      0\n",
      "2288      0\n",
      "11736     0\n",
      "7047      0\n",
      "         ..\n",
      "90344     1\n",
      "42038     1\n",
      "101295    1\n",
      "54444     1\n",
      "56028     1\n",
      "Name: fan_base, Length: 13743, dtype: int64, \n",
      "None, message_clsname='Pipeline', message=None)\n",
      "________________________________________________fit_transform_one - 0.0s, 0.0min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling sklearn.pipeline._fit_transform_one...\n",
      "_fit_transform_one(TruncatedSVD(n_components=500, random_state=42), <13743x2611 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 77247 stored elements in Compressed Sparse Row format>, \n",
      "6436      0\n",
      "1805      0\n",
      "2288      0\n",
      "11736     0\n",
      "7047      0\n",
      "         ..\n",
      "90344     1\n",
      "42038     1\n",
      "101295    1\n",
      "54444     1\n",
      "56028     1\n",
      "Name: fan_base, Length: 13743, dtype: int64, \n",
      "None, message_clsname='Pipeline', message=None)\n",
      "________________________________________________fit_transform_one - 1.8s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmpdlzmtq4_/joblib/sklearn/pipeline/_fit_transform_one/34158a0e020066669ba47f5a2e66f651\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmpdlzmtq4_/joblib/sklearn/pipeline/_fit_transform_one/1688a6574df6b34d692f889a011c4088\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmpdlzmtq4_/joblib/sklearn/pipeline/_fit_transform_one/a86da712acaa78d371c4fe74e5b4129a\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmpdlzmtq4_/joblib/sklearn/pipeline/_fit_transform_one/194d490c15417623e9e73a70faa8af1b\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.1s, 0.0min    : Loading _fit_transform_one from /tmp/tmpdlzmtq4_/joblib/sklearn/pipeline/_fit_transform_one/02f2e34dbc40f71c57daf7cae6d37661\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.1s, 0.0min    : Loading _fit_transform_one from /tmp/tmpdlzmtq4_/joblib/sklearn/pipeline/_fit_transform_one/d544a430f388a85d93e221cfa85a8fba\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmpdlzmtq4_/joblib/sklearn/pipeline/_fit_transform_one/0f73db2d45bf1af8d54e515a8a2c49e2\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.1s, 0.0min    : Loading _fit_transform_one from /tmp/tmpdlzmtq4_/joblib/sklearn/pipeline/_fit_transform_one/e1b77681a0b92237faa5a19d94462607\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.1s, 0.0min    : Loading _fit_transform_one from /tmp/tmpdlzmtq4_/joblib/sklearn/pipeline/_fit_transform_one/7557617569470e5adc437a6281ee2322\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmpdlzmtq4_/joblib/sklearn/pipeline/_fit_transform_one/34158a0e020066669ba47f5a2e66f651\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.1s, 0.0min    : Loading _fit_transform_one from /tmp/tmpdlzmtq4_/joblib/sklearn/pipeline/_fit_transform_one/1688a6574df6b34d692f889a011c4088\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.1s, 0.0min    : Loading _fit_transform_one from /tmp/tmpdlzmtq4_/joblib/sklearn/pipeline/_fit_transform_one/a86da712acaa78d371c4fe74e5b4129a\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmpdlzmtq4_/joblib/sklearn/pipeline/_fit_transform_one/194d490c15417623e9e73a70faa8af1b\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmpdlzmtq4_/joblib/sklearn/pipeline/_fit_transform_one/02f2e34dbc40f71c57daf7cae6d37661\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.1s, 0.0min    : Loading _fit_transform_one from /tmp/tmpdlzmtq4_/joblib/sklearn/pipeline/_fit_transform_one/d544a430f388a85d93e221cfa85a8fba\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmpdlzmtq4_/joblib/sklearn/pipeline/_fit_transform_one/0f73db2d45bf1af8d54e515a8a2c49e2\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmpdlzmtq4_/joblib/sklearn/pipeline/_fit_transform_one/e1b77681a0b92237faa5a19d94462607\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.1s, 0.0min    : Loading _fit_transform_one from /tmp/tmpdlzmtq4_/joblib/sklearn/pipeline/_fit_transform_one/7557617569470e5adc437a6281ee2322\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmpdlzmtq4_/joblib/sklearn/pipeline/_fit_transform_one/34158a0e020066669ba47f5a2e66f651\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmpdlzmtq4_/joblib/sklearn/pipeline/_fit_transform_one/1688a6574df6b34d692f889a011c4088\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmpdlzmtq4_/joblib/sklearn/pipeline/_fit_transform_one/a86da712acaa78d371c4fe74e5b4129a\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmpdlzmtq4_/joblib/sklearn/pipeline/_fit_transform_one/194d490c15417623e9e73a70faa8af1b\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmpdlzmtq4_/joblib/sklearn/pipeline/_fit_transform_one/02f2e34dbc40f71c57daf7cae6d37661\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.1s, 0.0min    : Loading _fit_transform_one from /tmp/tmpdlzmtq4_/joblib/sklearn/pipeline/_fit_transform_one/d544a430f388a85d93e221cfa85a8fba\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmpdlzmtq4_/joblib/sklearn/pipeline/_fit_transform_one/0f73db2d45bf1af8d54e515a8a2c49e2\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmpdlzmtq4_/joblib/sklearn/pipeline/_fit_transform_one/e1b77681a0b92237faa5a19d94462607\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.1s, 0.0min    : Loading _fit_transform_one from /tmp/tmpdlzmtq4_/joblib/sklearn/pipeline/_fit_transform_one/7557617569470e5adc437a6281ee2322\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling sklearn.pipeline._fit_transform_one...\n",
      "_fit_transform_one(CountVectorizer(analyzer=<function doc_tokens_lemma at 0x7fbc689839d0>,\n",
      "                min_df=3), \n",
      "6436      Lettttzzzz go Game time boom SuperBowl49 GoPat...\n",
      "1805      Is there a mercy rule in the NFL My god what a...\n",
      "2288      I don t like either team but for sake of compe...\n",
      "11736     Seahawks some sore losers bro tryna fight and ...\n",
      "7047      For the love of God please stop putting a Kard...\n",
      "                                ...                        \n",
      "70978           How many yall going to the send off GoHawks\n",
      "119004             I hope the Seahawks beat Seattle GoHawks\n",
      "112548              GoHawks watching the from Venezuela Yay\n",
      "22201                    That 25 guy is pretty good GoHawks\n",
      "108966    If you do not let the world know who you re ch...\n",
      "Name: title, Length: 20614, dtype: object, \n",
      "6436      0\n",
      "1805      0\n",
      "2288      0\n",
      "11736     0\n",
      "7047      0\n",
      "         ..\n",
      "70978     1\n",
      "119004    1\n",
      "112548    1\n",
      "22201     1\n",
      "108966    1\n",
      "Name: fan_base, Length: 20614, dtype: int64, \n",
      "None, message_clsname='Pipeline', message=None)\n",
      "________________________________________________fit_transform_one - 8.0s, 0.1min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling sklearn.pipeline._fit_transform_one...\n",
      "_fit_transform_one(TfidfTransformer(), <20614x3448 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 117704 stored elements in Compressed Sparse Row format>, \n",
      "6436      0\n",
      "1805      0\n",
      "2288      0\n",
      "11736     0\n",
      "7047      0\n",
      "         ..\n",
      "70978     1\n",
      "119004    1\n",
      "112548    1\n",
      "22201     1\n",
      "108966    1\n",
      "Name: fan_base, Length: 20614, dtype: int64, \n",
      "None, message_clsname='Pipeline', message=None)\n",
      "________________________________________________fit_transform_one - 0.0s, 0.0min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling sklearn.pipeline._fit_transform_one...\n",
      "_fit_transform_one(TruncatedSVD(n_components=500, random_state=42), <20614x3448 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 117704 stored elements in Compressed Sparse Row format>, \n",
      "6436      0\n",
      "1805      0\n",
      "2288      0\n",
      "11736     0\n",
      "7047      0\n",
      "         ..\n",
      "70978     1\n",
      "119004    1\n",
      "112548    1\n",
      "22201     1\n",
      "108966    1\n",
      "Name: fan_base, Length: 20614, dtype: int64, \n",
      "None, message_clsname='Pipeline', message=None)\n",
      "________________________________________________fit_transform_one - 2.7s, 0.0min\n"
     ]
    }
   ],
   "source": [
    "grid = GridSearchCV(pipeline, cv=3, n_jobs=1, param_grid=param_grid, scoring='accuracy')\n",
    "grid.fit(train.title, train.fan_base)\n",
    "rmtree(cachedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "019966fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_clf</th>\n",
       "      <th>param_reduce_dim</th>\n",
       "      <th>param_vect</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.308131</td>\n",
       "      <td>0.082243</td>\n",
       "      <td>2.752050</td>\n",
       "      <td>0.024338</td>\n",
       "      <td>SVC(C=10, kernel='linear', random_state=42)</td>\n",
       "      <td>TruncatedSVD(n_components=500, random_state=42)</td>\n",
       "      <td>CountVectorizer(analyzer=&lt;function doc_tokens_...</td>\n",
       "      <td>{'clf': SVC(C=10, kernel='linear', random_stat...</td>\n",
       "      <td>0.999563</td>\n",
       "      <td>0.999418</td>\n",
       "      <td>0.999418</td>\n",
       "      <td>0.999466</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.211244</td>\n",
       "      <td>0.005096</td>\n",
       "      <td>2.636499</td>\n",
       "      <td>0.009563</td>\n",
       "      <td>LogisticRegression(C=10, random_state=42)</td>\n",
       "      <td>TruncatedSVD(n_components=500, random_state=42)</td>\n",
       "      <td>CountVectorizer(analyzer=&lt;function doc_tokens_...</td>\n",
       "      <td>{'clf': LogisticRegression(C=10, random_state=...</td>\n",
       "      <td>0.999272</td>\n",
       "      <td>0.999127</td>\n",
       "      <td>0.999127</td>\n",
       "      <td>0.999175</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.188560</td>\n",
       "      <td>0.049809</td>\n",
       "      <td>2.665139</td>\n",
       "      <td>0.018704</td>\n",
       "      <td>GaussianNB()</td>\n",
       "      <td>TruncatedSVD(n_components=500, random_state=42)</td>\n",
       "      <td>CountVectorizer(analyzer=&lt;function doc_tokens_...</td>\n",
       "      <td>{'clf': GaussianNB(), 'reduce_dim': TruncatedS...</td>\n",
       "      <td>0.687864</td>\n",
       "      <td>0.684762</td>\n",
       "      <td>0.667152</td>\n",
       "      <td>0.679926</td>\n",
       "      <td>0.009121</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12.995217</td>\n",
       "      <td>0.083480</td>\n",
       "      <td>2.703887</td>\n",
       "      <td>0.012373</td>\n",
       "      <td>RandomForestClassifier(max_depth=10, random_st...</td>\n",
       "      <td>TruncatedSVD(n_components=500, random_state=42)</td>\n",
       "      <td>CountVectorizer(analyzer=&lt;function doc_tokens_...</td>\n",
       "      <td>{'clf': RandomForestClassifier(max_depth=10, r...</td>\n",
       "      <td>0.997526</td>\n",
       "      <td>0.997526</td>\n",
       "      <td>0.997089</td>\n",
       "      <td>0.997380</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0       8.308131      0.082243         2.752050        0.024338   \n",
       "1       0.211244      0.005096         2.636499        0.009563   \n",
       "2       0.188560      0.049809         2.665139        0.018704   \n",
       "3      12.995217      0.083480         2.703887        0.012373   \n",
       "\n",
       "                                           param_clf  \\\n",
       "0        SVC(C=10, kernel='linear', random_state=42)   \n",
       "1          LogisticRegression(C=10, random_state=42)   \n",
       "2                                       GaussianNB()   \n",
       "3  RandomForestClassifier(max_depth=10, random_st...   \n",
       "\n",
       "                                  param_reduce_dim  \\\n",
       "0  TruncatedSVD(n_components=500, random_state=42)   \n",
       "1  TruncatedSVD(n_components=500, random_state=42)   \n",
       "2  TruncatedSVD(n_components=500, random_state=42)   \n",
       "3  TruncatedSVD(n_components=500, random_state=42)   \n",
       "\n",
       "                                          param_vect  \\\n",
       "0  CountVectorizer(analyzer=<function doc_tokens_...   \n",
       "1  CountVectorizer(analyzer=<function doc_tokens_...   \n",
       "2  CountVectorizer(analyzer=<function doc_tokens_...   \n",
       "3  CountVectorizer(analyzer=<function doc_tokens_...   \n",
       "\n",
       "                                              params  split0_test_score  \\\n",
       "0  {'clf': SVC(C=10, kernel='linear', random_stat...           0.999563   \n",
       "1  {'clf': LogisticRegression(C=10, random_state=...           0.999272   \n",
       "2  {'clf': GaussianNB(), 'reduce_dim': TruncatedS...           0.687864   \n",
       "3  {'clf': RandomForestClassifier(max_depth=10, r...           0.997526   \n",
       "\n",
       "   split1_test_score  split2_test_score  mean_test_score  std_test_score  \\\n",
       "0           0.999418           0.999418         0.999466        0.000069   \n",
       "1           0.999127           0.999127         0.999175        0.000069   \n",
       "2           0.684762           0.667152         0.679926        0.009121   \n",
       "3           0.997526           0.997089         0.997380        0.000206   \n",
       "\n",
       "   rank_test_score  \n",
       "0                1  \n",
       "1                2  \n",
       "2                4  \n",
       "3                3  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = pd.DataFrame(grid.cv_results_)\n",
    "pd.DataFrame(grid.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "30d025cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The index of 5 best combination:{1: 0, 2: 1, 4: 2, 3: 3}\n"
     ]
    }
   ],
   "source": [
    "best_list = {}\n",
    "k = 1\n",
    "for i,row in enumerate(result['rank_test_score']):\n",
    "    if row >= 1 and row <= 5:\n",
    "        best_list[row] = i\n",
    "    \n",
    "print(\"The index of 5 best combination:{}\".format(best_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bde35e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "rank_test_score:\n",
      "1\n",
      "____________________\n",
      "mean_test_score:\n",
      "0.9994663773615744\n",
      "____________________\n",
      "model parameters:\n",
      "{'clf': SVC(C=10, kernel='linear', random_state=42), 'reduce_dim': TruncatedSVD(n_components=500, random_state=42), 'vect': CountVectorizer(analyzer=<function doc_tokens_lemma at 0x7fbc689839d0>,\n",
      "                min_df=3)}\n",
      "________________________________________________________________________________\n",
      "________________________________________________________________________________\n",
      "rank_test_score:\n",
      "2\n",
      "____________________\n",
      "mean_test_score:\n",
      "0.9991753130349887\n",
      "____________________\n",
      "model parameters:\n",
      "{'clf': LogisticRegression(C=10, random_state=42), 'reduce_dim': TruncatedSVD(n_components=500, random_state=42), 'vect': CountVectorizer(analyzer=<function doc_tokens_lemma at 0x7fbc689839d0>,\n",
      "                min_df=3)}\n",
      "________________________________________________________________________________\n",
      "________________________________________________________________________________\n",
      "rank_test_score:\n",
      "3\n",
      "____________________\n",
      "mean_test_score:\n",
      "0.9973804140012009\n",
      "____________________\n",
      "model parameters:\n",
      "{'clf': RandomForestClassifier(max_depth=10, random_state=42), 'reduce_dim': TruncatedSVD(n_components=500, random_state=42), 'vect': CountVectorizer(analyzer=<function doc_tokens_lemma at 0x7fbc689839d0>,\n",
      "                min_df=3)}\n",
      "________________________________________________________________________________\n",
      "________________________________________________________________________________\n",
      "rank_test_score:\n",
      "4\n",
      "____________________\n",
      "mean_test_score:\n",
      "0.6799258786302279\n",
      "____________________\n",
      "model parameters:\n",
      "{'clf': GaussianNB(), 'reduce_dim': TruncatedSVD(n_components=500, random_state=42), 'vect': CountVectorizer(analyzer=<function doc_tokens_lemma at 0x7fbc689839d0>,\n",
      "                min_df=3)}\n",
      "________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "while i <= 4:\n",
    "    index = best_list[i]\n",
    "    print(\"_\" * 80)\n",
    "    print(\"rank_test_score:\")\n",
    "    print(result['rank_test_score'][index]) \n",
    "    print(\"_\" * 20)\n",
    "    print(\"mean_test_score:\")\n",
    "    print(result['mean_test_score'][index])     \n",
    "    print(\"_\" * 20)\n",
    "    print(\"model parameters:\")\n",
    "    print(result['params'][index])  \n",
    "    print(\"_\" * 80)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "72c83dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Try by removing keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e6ddc3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cachedir = mkdtemp()\n",
    "memory = joblib.Memory(location=cachedir, verbose=10)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(min_df=3, analyzer=doc_tokens_lemma)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('reduce_dim', TruncatedSVD(random_state=42)),\n",
    "    ('clf', GaussianNB()),\n",
    "],\n",
    "memory=memory\n",
    ")\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        'vect': [CountVectorizer(min_df=3, analyzer=doc_tokens_lemma),\n",
    "        ],\n",
    "        'reduce_dim': [\n",
    "                        TruncatedSVD(n_components=500, random_state=42),\n",
    "                         NMF(n_components=500, init='random', random_state=42),\n",
    "        ],\n",
    "        'clf': [\n",
    "                SVC(kernel='linear', C=10, random_state=42),\n",
    "                 LogisticRegression(penalty='l2', C=10, random_state=42),\n",
    "                 GaussianNB(),\n",
    "                 RandomForestClassifier(max_depth=10, random_state=42)\n",
    "        ],\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "56c9a07d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "[Memory] Calling sklearn.pipeline._fit_transform_one...\n",
      "_fit_transform_one(CountVectorizer(analyzer=<function doc_tokens_lemma at 0x7fbc68983430>,\n",
      "                min_df=3), \n",
      "7089      The Seahawks better not win this Super Bowl Go...\n",
      "8473      Yall seahawk fans quiet yall aint making no no...\n",
      "12112                    patriots GoPatriots Finish the Job\n",
      "12181                         SuperBowl reaction GoPatriots\n",
      "8794                      Don t let me down Pats gopatriots\n",
      "                                ...                        \n",
      "70978           How many yall going to the send off GoHawks\n",
      "119004             I hope the Seahawks beat Seattle GoHawks\n",
      "112548              GoHawks watching the from Venezuela Yay\n",
      "22201                    That 25 guy is pretty good GoHawks\n",
      "108966    If you do not let the world know who you re ch...\n",
      "Name: title, Length: 13742, dtype: object, \n",
      "7089      0\n",
      "8473      0\n",
      "12112     0\n",
      "12181     0\n",
      "8794      0\n",
      "         ..\n",
      "70978     1\n",
      "119004    1\n",
      "112548    1\n",
      "22201     1\n",
      "108966    1\n",
      "Name: fan_base, Length: 13742, dtype: int64, \n",
      "None, message_clsname='Pipeline', message=None)\n",
      "________________________________________________fit_transform_one - 4.8s, 0.1min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling sklearn.pipeline._fit_transform_one...\n",
      "_fit_transform_one(TfidfTransformer(), <13742x2590 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 61744 stored elements in Compressed Sparse Row format>, \n",
      "7089      0\n",
      "8473      0\n",
      "12112     0\n",
      "12181     0\n",
      "8794      0\n",
      "         ..\n",
      "70978     1\n",
      "119004    1\n",
      "112548    1\n",
      "22201     1\n",
      "108966    1\n",
      "Name: fan_base, Length: 13742, dtype: int64, \n",
      "None, message_clsname='Pipeline', message=None)\n",
      "________________________________________________fit_transform_one - 0.0s, 0.0min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling sklearn.pipeline._fit_transform_one...\n",
      "_fit_transform_one(TruncatedSVD(n_components=500, random_state=42), <13742x2590 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 61744 stored elements in Compressed Sparse Row format>, \n",
      "7089      0\n",
      "8473      0\n",
      "12112     0\n",
      "12181     0\n",
      "8794      0\n",
      "         ..\n",
      "70978     1\n",
      "119004    1\n",
      "112548    1\n",
      "22201     1\n",
      "108966    1\n",
      "Name: fan_base, Length: 13742, dtype: int64, \n",
      "None, message_clsname='Pipeline', message=None)\n",
      "________________________________________________fit_transform_one - 1.8s, 0.0min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling sklearn.pipeline._fit_transform_one...\n",
      "_fit_transform_one(CountVectorizer(analyzer=<function doc_tokens_lemma at 0x7fbc68983430>,\n",
      "                min_df=3), \n",
      "6436      Lettttzzzz go Game time boom SuperBowl49 GoPat...\n",
      "1805      Is there a mercy rule in the NFL My god what a...\n",
      "2288      I don t like either team but for sake of compe...\n",
      "11736     Seahawks some sore losers bro tryna fight and ...\n",
      "7047      For the love of God please stop putting a Kard...\n",
      "                                ...                        \n",
      "70978           How many yall going to the send off GoHawks\n",
      "119004             I hope the Seahawks beat Seattle GoHawks\n",
      "112548              GoHawks watching the from Venezuela Yay\n",
      "22201                    That 25 guy is pretty good GoHawks\n",
      "108966    If you do not let the world know who you re ch...\n",
      "Name: title, Length: 13743, dtype: object, \n",
      "6436      0\n",
      "1805      0\n",
      "2288      0\n",
      "11736     0\n",
      "7047      0\n",
      "         ..\n",
      "70978     1\n",
      "119004    1\n",
      "112548    1\n",
      "22201     1\n",
      "108966    1\n",
      "Name: fan_base, Length: 13743, dtype: int64, \n",
      "None, message_clsname='Pipeline', message=None)\n",
      "________________________________________________fit_transform_one - 4.8s, 0.1min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling sklearn.pipeline._fit_transform_one...\n",
      "_fit_transform_one(TfidfTransformer(), <13743x2605 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 61920 stored elements in Compressed Sparse Row format>, \n",
      "6436      0\n",
      "1805      0\n",
      "2288      0\n",
      "11736     0\n",
      "7047      0\n",
      "         ..\n",
      "70978     1\n",
      "119004    1\n",
      "112548    1\n",
      "22201     1\n",
      "108966    1\n",
      "Name: fan_base, Length: 13743, dtype: int64, \n",
      "None, message_clsname='Pipeline', message=None)\n",
      "________________________________________________fit_transform_one - 0.0s, 0.0min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling sklearn.pipeline._fit_transform_one...\n",
      "_fit_transform_one(TruncatedSVD(n_components=500, random_state=42), <13743x2605 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 61920 stored elements in Compressed Sparse Row format>, \n",
      "6436      0\n",
      "1805      0\n",
      "2288      0\n",
      "11736     0\n",
      "7047      0\n",
      "         ..\n",
      "70978     1\n",
      "119004    1\n",
      "112548    1\n",
      "22201     1\n",
      "108966    1\n",
      "Name: fan_base, Length: 13743, dtype: int64, \n",
      "None, message_clsname='Pipeline', message=None)\n",
      "________________________________________________fit_transform_one - 1.7s, 0.0min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling sklearn.pipeline._fit_transform_one...\n",
      "_fit_transform_one(CountVectorizer(analyzer=<function doc_tokens_lemma at 0x7fbc68983430>,\n",
      "                min_df=3), \n",
      "6436      Lettttzzzz go Game time boom SuperBowl49 GoPat...\n",
      "1805      Is there a mercy rule in the NFL My god what a...\n",
      "2288      I don t like either team but for sake of compe...\n",
      "11736     Seahawks some sore losers bro tryna fight and ...\n",
      "7047      For the love of God please stop putting a Kard...\n",
      "                                ...                        \n",
      "90344     We got that beer game going strong NFL GoHawks...\n",
      "42038     PACKERS ARE KEARSED SeahawksvsPackers Seahawks...\n",
      "101295             We have arrived 12thMan Seahawks GoHawks\n",
      "54444     What of country do you think will root for Sea...\n",
      "56028     Wilson s 4 picks were all intended for Kearse ...\n",
      "Name: title, Length: 13743, dtype: object, \n",
      "6436      0\n",
      "1805      0\n",
      "2288      0\n",
      "11736     0\n",
      "7047      0\n",
      "         ..\n",
      "90344     1\n",
      "42038     1\n",
      "101295    1\n",
      "54444     1\n",
      "56028     1\n",
      "Name: fan_base, Length: 13743, dtype: int64, \n",
      "None, message_clsname='Pipeline', message=None)\n",
      "________________________________________________fit_transform_one - 4.7s, 0.1min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling sklearn.pipeline._fit_transform_one...\n",
      "_fit_transform_one(TfidfTransformer(), <13743x2612 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 62086 stored elements in Compressed Sparse Row format>, \n",
      "6436      0\n",
      "1805      0\n",
      "2288      0\n",
      "11736     0\n",
      "7047      0\n",
      "         ..\n",
      "90344     1\n",
      "42038     1\n",
      "101295    1\n",
      "54444     1\n",
      "56028     1\n",
      "Name: fan_base, Length: 13743, dtype: int64, \n",
      "None, message_clsname='Pipeline', message=None)\n",
      "________________________________________________fit_transform_one - 0.0s, 0.0min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling sklearn.pipeline._fit_transform_one...\n",
      "_fit_transform_one(TruncatedSVD(n_components=500, random_state=42), <13743x2612 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 62086 stored elements in Compressed Sparse Row format>, \n",
      "6436      0\n",
      "1805      0\n",
      "2288      0\n",
      "11736     0\n",
      "7047      0\n",
      "         ..\n",
      "90344     1\n",
      "42038     1\n",
      "101295    1\n",
      "54444     1\n",
      "56028     1\n",
      "Name: fan_base, Length: 13743, dtype: int64, \n",
      "None, message_clsname='Pipeline', message=None)\n",
      "________________________________________________fit_transform_one - 1.8s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/34158a0e020066669ba47f5a2e66f651\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/dccef523d46fd62e8e61b0bc98cc6dec\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling sklearn.pipeline._fit_transform_one...\n",
      "_fit_transform_one(NMF(init='random', n_components=500, random_state=42), <13742x2590 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 61744 stored elements in Compressed Sparse Row format>, \n",
      "7089      0\n",
      "8473      0\n",
      "12112     0\n",
      "12181     0\n",
      "8794      0\n",
      "         ..\n",
      "70978     1\n",
      "119004    1\n",
      "112548    1\n",
      "22201     1\n",
      "108966    1\n",
      "Name: fan_base, Length: 13742, dtype: int64, \n",
      "None, message_clsname='Pipeline', message=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______________________________________________fit_transform_one - 430.9s, 7.2min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/194d490c15417623e9e73a70faa8af1b\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/c6de3af9cca9fc6c1a0fc62812e5bebb\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling sklearn.pipeline._fit_transform_one...\n",
      "_fit_transform_one(NMF(init='random', n_components=500, random_state=42), <13743x2605 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 61920 stored elements in Compressed Sparse Row format>, \n",
      "6436      0\n",
      "1805      0\n",
      "2288      0\n",
      "11736     0\n",
      "7047      0\n",
      "         ..\n",
      "70978     1\n",
      "119004    1\n",
      "112548    1\n",
      "22201     1\n",
      "108966    1\n",
      "Name: fan_base, Length: 13743, dtype: int64, \n",
      "None, message_clsname='Pipeline', message=None)\n",
      "______________________________________________fit_transform_one - 541.5s, 9.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/0f73db2d45bf1af8d54e515a8a2c49e2\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/eb3dc91705478cc9f612e8142141f51a\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling sklearn.pipeline._fit_transform_one...\n",
      "_fit_transform_one(NMF(init='random', n_components=500, random_state=42), <13743x2612 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 62086 stored elements in Compressed Sparse Row format>, \n",
      "6436      0\n",
      "1805      0\n",
      "2288      0\n",
      "11736     0\n",
      "7047      0\n",
      "         ..\n",
      "90344     1\n",
      "42038     1\n",
      "101295    1\n",
      "54444     1\n",
      "56028     1\n",
      "Name: fan_base, Length: 13743, dtype: int64, \n",
      "None, message_clsname='Pipeline', message=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shivam/.local/lib/python3.8/site-packages/sklearn/decomposition/_nmf.py:1637: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____________________________________________fit_transform_one - 644.1s, 10.7min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/34158a0e020066669ba47f5a2e66f651\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/dccef523d46fd62e8e61b0bc98cc6dec\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/833d29a9d1f3df179332ede20b01b213\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/194d490c15417623e9e73a70faa8af1b\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.1s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/c6de3af9cca9fc6c1a0fc62812e5bebb\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.1s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/688d549e69dcfab0c10deec6013bcf75\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/0f73db2d45bf1af8d54e515a8a2c49e2\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.1s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/eb3dc91705478cc9f612e8142141f51a\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.1s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/ad0aaa286ebd0a1370c85be5dc1ed2ac\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/34158a0e020066669ba47f5a2e66f651\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.1s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/dccef523d46fd62e8e61b0bc98cc6dec\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.1s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/90f60c3d4f12d0b72de12eb97d5a3ccd\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shivam/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/194d490c15417623e9e73a70faa8af1b\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.1s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/c6de3af9cca9fc6c1a0fc62812e5bebb\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.1s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/0af5c04ffb38e90368e2bfdc3d6e7d6d\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shivam/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/0f73db2d45bf1af8d54e515a8a2c49e2\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.1s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/eb3dc91705478cc9f612e8142141f51a\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.1s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/35eb7cb7135cbc32991ace40dc148da6\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shivam/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/34158a0e020066669ba47f5a2e66f651\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.1s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/dccef523d46fd62e8e61b0bc98cc6dec\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.1s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/833d29a9d1f3df179332ede20b01b213\n",
      "___________________________________fit_transform_one cache loaded - 0.1s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/194d490c15417623e9e73a70faa8af1b\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.1s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/c6de3af9cca9fc6c1a0fc62812e5bebb\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.1s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/688d549e69dcfab0c10deec6013bcf75\n",
      "___________________________________fit_transform_one cache loaded - 0.1s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/0f73db2d45bf1af8d54e515a8a2c49e2\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/eb3dc91705478cc9f612e8142141f51a\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.1s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/ad0aaa286ebd0a1370c85be5dc1ed2ac\n",
      "___________________________________fit_transform_one cache loaded - 0.1s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/34158a0e020066669ba47f5a2e66f651\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/dccef523d46fd62e8e61b0bc98cc6dec\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/90f60c3d4f12d0b72de12eb97d5a3ccd\n",
      "___________________________________fit_transform_one cache loaded - 0.2s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/194d490c15417623e9e73a70faa8af1b\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/c6de3af9cca9fc6c1a0fc62812e5bebb\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.1s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/0af5c04ffb38e90368e2bfdc3d6e7d6d\n",
      "___________________________________fit_transform_one cache loaded - 0.2s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/0f73db2d45bf1af8d54e515a8a2c49e2\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/eb3dc91705478cc9f612e8142141f51a\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/35eb7cb7135cbc32991ace40dc148da6\n",
      "___________________________________fit_transform_one cache loaded - 0.2s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/34158a0e020066669ba47f5a2e66f651\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.1s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/dccef523d46fd62e8e61b0bc98cc6dec\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.1s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/833d29a9d1f3df179332ede20b01b213\n",
      "___________________________________fit_transform_one cache loaded - 0.2s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/194d490c15417623e9e73a70faa8af1b\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/c6de3af9cca9fc6c1a0fc62812e5bebb\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.1s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/688d549e69dcfab0c10deec6013bcf75\n",
      "___________________________________fit_transform_one cache loaded - 0.1s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/0f73db2d45bf1af8d54e515a8a2c49e2\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.1s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/eb3dc91705478cc9f612e8142141f51a\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.1s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/ad0aaa286ebd0a1370c85be5dc1ed2ac\n",
      "___________________________________fit_transform_one cache loaded - 0.1s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/34158a0e020066669ba47f5a2e66f651\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/dccef523d46fd62e8e61b0bc98cc6dec\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.1s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/90f60c3d4f12d0b72de12eb97d5a3ccd\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/194d490c15417623e9e73a70faa8af1b\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/c6de3af9cca9fc6c1a0fc62812e5bebb\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.1s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/0af5c04ffb38e90368e2bfdc3d6e7d6d\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/0f73db2d45bf1af8d54e515a8a2c49e2\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/eb3dc91705478cc9f612e8142141f51a\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/35eb7cb7135cbc32991ace40dc148da6\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "[Memory] Calling sklearn.pipeline._fit_transform_one...\n",
      "_fit_transform_one(CountVectorizer(analyzer=<function doc_tokens_lemma at 0x7fbc68983430>,\n",
      "                min_df=3), \n",
      "6436      Lettttzzzz go Game time boom SuperBowl49 GoPat...\n",
      "1805      Is there a mercy rule in the NFL My god what a...\n",
      "2288      I don t like either team but for sake of compe...\n",
      "11736     Seahawks some sore losers bro tryna fight and ...\n",
      "7047      For the love of God please stop putting a Kard...\n",
      "                                ...                        \n",
      "70978           How many yall going to the send off GoHawks\n",
      "119004             I hope the Seahawks beat Seattle GoHawks\n",
      "112548              GoHawks watching the from Venezuela Yay\n",
      "22201                    That 25 guy is pretty good GoHawks\n",
      "108966    If you do not let the world know who you re ch...\n",
      "Name: title, Length: 20614, dtype: object, \n",
      "6436      0\n",
      "1805      0\n",
      "2288      0\n",
      "11736     0\n",
      "7047      0\n",
      "         ..\n",
      "70978     1\n",
      "119004    1\n",
      "112548    1\n",
      "22201     1\n",
      "108966    1\n",
      "Name: fan_base, Length: 20614, dtype: int64, \n",
      "None, message_clsname='Pipeline', message=None)\n",
      "________________________________________________fit_transform_one - 7.1s, 0.1min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling sklearn.pipeline._fit_transform_one...\n",
      "_fit_transform_one(TfidfTransformer(), <20614x3452 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 94968 stored elements in Compressed Sparse Row format>, \n",
      "6436      0\n",
      "1805      0\n",
      "2288      0\n",
      "11736     0\n",
      "7047      0\n",
      "         ..\n",
      "70978     1\n",
      "119004    1\n",
      "112548    1\n",
      "22201     1\n",
      "108966    1\n",
      "Name: fan_base, Length: 20614, dtype: int64, \n",
      "None, message_clsname='Pipeline', message=None)\n",
      "________________________________________________fit_transform_one - 0.0s, 0.0min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling sklearn.pipeline._fit_transform_one...\n",
      "_fit_transform_one(NMF(init='random', n_components=500, random_state=42), <20614x3452 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 94968 stored elements in Compressed Sparse Row format>, \n",
      "6436      0\n",
      "1805      0\n",
      "2288      0\n",
      "11736     0\n",
      "7047      0\n",
      "         ..\n",
      "70978     1\n",
      "119004    1\n",
      "112548    1\n",
      "22201     1\n",
      "108966    1\n",
      "Name: fan_base, Length: 20614, dtype: int64, \n",
      "None, message_clsname='Pipeline', message=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shivam/.local/lib/python3.8/site-packages/sklearn/decomposition/_nmf.py:1637: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____________________________________________fit_transform_one - 896.6s, 14.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shivam/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "grid2 = GridSearchCV(pipeline, cv=3, n_jobs=1, param_grid=param_grid, scoring='accuracy')\n",
    "grid2.fit(train.title, train.fan_base)\n",
    "rmtree(cachedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "6dbc03de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_clf</th>\n",
       "      <th>param_reduce_dim</th>\n",
       "      <th>param_vect</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37.267668</td>\n",
       "      <td>0.251887</td>\n",
       "      <td>11.704723</td>\n",
       "      <td>0.099588</td>\n",
       "      <td>SVC(C=10, kernel='linear', random_state=42)</td>\n",
       "      <td>TruncatedSVD(n_components=500, random_state=42)</td>\n",
       "      <td>CountVectorizer(analyzer=&lt;function doc_tokens_...</td>\n",
       "      <td>{'clf': SVC(C=10, kernel='linear', random_stat...</td>\n",
       "      <td>0.747963</td>\n",
       "      <td>0.753020</td>\n",
       "      <td>0.738029</td>\n",
       "      <td>0.746337</td>\n",
       "      <td>0.006227</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570.100207</td>\n",
       "      <td>88.016414</td>\n",
       "      <td>77.853300</td>\n",
       "      <td>61.368086</td>\n",
       "      <td>SVC(C=10, kernel='linear', random_state=42)</td>\n",
       "      <td>NMF(init='random', n_components=500, random_st...</td>\n",
       "      <td>CountVectorizer(analyzer=&lt;function doc_tokens_...</td>\n",
       "      <td>{'clf': SVC(C=10, kernel='linear', random_stat...</td>\n",
       "      <td>0.748545</td>\n",
       "      <td>0.752001</td>\n",
       "      <td>0.743123</td>\n",
       "      <td>0.747890</td>\n",
       "      <td>0.003654</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.572136</td>\n",
       "      <td>0.119720</td>\n",
       "      <td>2.342068</td>\n",
       "      <td>0.006577</td>\n",
       "      <td>LogisticRegression(C=10, random_state=42)</td>\n",
       "      <td>TruncatedSVD(n_components=500, random_state=42)</td>\n",
       "      <td>CountVectorizer(analyzer=&lt;function doc_tokens_...</td>\n",
       "      <td>{'clf': LogisticRegression(C=10, random_state=...</td>\n",
       "      <td>0.750582</td>\n",
       "      <td>0.753165</td>\n",
       "      <td>0.743123</td>\n",
       "      <td>0.748957</td>\n",
       "      <td>0.004258</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.661059</td>\n",
       "      <td>0.044667</td>\n",
       "      <td>68.292213</td>\n",
       "      <td>60.461573</td>\n",
       "      <td>LogisticRegression(C=10, random_state=42)</td>\n",
       "      <td>NMF(init='random', n_components=500, random_st...</td>\n",
       "      <td>CountVectorizer(analyzer=&lt;function doc_tokens_...</td>\n",
       "      <td>{'clf': LogisticRegression(C=10, random_state=...</td>\n",
       "      <td>0.757421</td>\n",
       "      <td>0.750255</td>\n",
       "      <td>0.745306</td>\n",
       "      <td>0.750994</td>\n",
       "      <td>0.004974</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.316643</td>\n",
       "      <td>0.034054</td>\n",
       "      <td>2.340587</td>\n",
       "      <td>0.007816</td>\n",
       "      <td>GaussianNB()</td>\n",
       "      <td>TruncatedSVD(n_components=500, random_state=42)</td>\n",
       "      <td>CountVectorizer(analyzer=&lt;function doc_tokens_...</td>\n",
       "      <td>{'clf': GaussianNB(), 'reduce_dim': TruncatedS...</td>\n",
       "      <td>0.635623</td>\n",
       "      <td>0.630039</td>\n",
       "      <td>0.626110</td>\n",
       "      <td>0.630591</td>\n",
       "      <td>0.003903</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.403428</td>\n",
       "      <td>0.053601</td>\n",
       "      <td>67.056438</td>\n",
       "      <td>60.749883</td>\n",
       "      <td>GaussianNB()</td>\n",
       "      <td>NMF(init='random', n_components=500, random_st...</td>\n",
       "      <td>CountVectorizer(analyzer=&lt;function doc_tokens_...</td>\n",
       "      <td>{'clf': GaussianNB(), 'reduce_dim': NMF(init='...</td>\n",
       "      <td>0.724243</td>\n",
       "      <td>0.732062</td>\n",
       "      <td>0.727259</td>\n",
       "      <td>0.727855</td>\n",
       "      <td>0.003220</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>14.516446</td>\n",
       "      <td>0.075953</td>\n",
       "      <td>2.375185</td>\n",
       "      <td>0.007645</td>\n",
       "      <td>RandomForestClassifier(max_depth=10, random_st...</td>\n",
       "      <td>TruncatedSVD(n_components=500, random_state=42)</td>\n",
       "      <td>CountVectorizer(analyzer=&lt;function doc_tokens_...</td>\n",
       "      <td>{'clf': RandomForestClassifier(max_depth=10, r...</td>\n",
       "      <td>0.726572</td>\n",
       "      <td>0.733372</td>\n",
       "      <td>0.718527</td>\n",
       "      <td>0.726157</td>\n",
       "      <td>0.006068</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.566802</td>\n",
       "      <td>0.025442</td>\n",
       "      <td>67.548493</td>\n",
       "      <td>60.777820</td>\n",
       "      <td>RandomForestClassifier(max_depth=10, random_st...</td>\n",
       "      <td>NMF(init='random', n_components=500, random_st...</td>\n",
       "      <td>CountVectorizer(analyzer=&lt;function doc_tokens_...</td>\n",
       "      <td>{'clf': RandomForestClassifier(max_depth=10, r...</td>\n",
       "      <td>0.715367</td>\n",
       "      <td>0.710086</td>\n",
       "      <td>0.716490</td>\n",
       "      <td>0.713981</td>\n",
       "      <td>0.002792</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0      37.267668      0.251887        11.704723        0.099588   \n",
       "1     570.100207     88.016414        77.853300       61.368086   \n",
       "2       0.572136      0.119720         2.342068        0.006577   \n",
       "3       0.661059      0.044667        68.292213       60.461573   \n",
       "4       0.316643      0.034054         2.340587        0.007816   \n",
       "5       0.403428      0.053601        67.056438       60.749883   \n",
       "6      14.516446      0.075953         2.375185        0.007645   \n",
       "7       1.566802      0.025442        67.548493       60.777820   \n",
       "\n",
       "                                           param_clf  \\\n",
       "0        SVC(C=10, kernel='linear', random_state=42)   \n",
       "1        SVC(C=10, kernel='linear', random_state=42)   \n",
       "2          LogisticRegression(C=10, random_state=42)   \n",
       "3          LogisticRegression(C=10, random_state=42)   \n",
       "4                                       GaussianNB()   \n",
       "5                                       GaussianNB()   \n",
       "6  RandomForestClassifier(max_depth=10, random_st...   \n",
       "7  RandomForestClassifier(max_depth=10, random_st...   \n",
       "\n",
       "                                    param_reduce_dim  \\\n",
       "0    TruncatedSVD(n_components=500, random_state=42)   \n",
       "1  NMF(init='random', n_components=500, random_st...   \n",
       "2    TruncatedSVD(n_components=500, random_state=42)   \n",
       "3  NMF(init='random', n_components=500, random_st...   \n",
       "4    TruncatedSVD(n_components=500, random_state=42)   \n",
       "5  NMF(init='random', n_components=500, random_st...   \n",
       "6    TruncatedSVD(n_components=500, random_state=42)   \n",
       "7  NMF(init='random', n_components=500, random_st...   \n",
       "\n",
       "                                          param_vect  \\\n",
       "0  CountVectorizer(analyzer=<function doc_tokens_...   \n",
       "1  CountVectorizer(analyzer=<function doc_tokens_...   \n",
       "2  CountVectorizer(analyzer=<function doc_tokens_...   \n",
       "3  CountVectorizer(analyzer=<function doc_tokens_...   \n",
       "4  CountVectorizer(analyzer=<function doc_tokens_...   \n",
       "5  CountVectorizer(analyzer=<function doc_tokens_...   \n",
       "6  CountVectorizer(analyzer=<function doc_tokens_...   \n",
       "7  CountVectorizer(analyzer=<function doc_tokens_...   \n",
       "\n",
       "                                              params  split0_test_score  \\\n",
       "0  {'clf': SVC(C=10, kernel='linear', random_stat...           0.747963   \n",
       "1  {'clf': SVC(C=10, kernel='linear', random_stat...           0.748545   \n",
       "2  {'clf': LogisticRegression(C=10, random_state=...           0.750582   \n",
       "3  {'clf': LogisticRegression(C=10, random_state=...           0.757421   \n",
       "4  {'clf': GaussianNB(), 'reduce_dim': TruncatedS...           0.635623   \n",
       "5  {'clf': GaussianNB(), 'reduce_dim': NMF(init='...           0.724243   \n",
       "6  {'clf': RandomForestClassifier(max_depth=10, r...           0.726572   \n",
       "7  {'clf': RandomForestClassifier(max_depth=10, r...           0.715367   \n",
       "\n",
       "   split1_test_score  split2_test_score  mean_test_score  std_test_score  \\\n",
       "0           0.753020           0.738029         0.746337        0.006227   \n",
       "1           0.752001           0.743123         0.747890        0.003654   \n",
       "2           0.753165           0.743123         0.748957        0.004258   \n",
       "3           0.750255           0.745306         0.750994        0.004974   \n",
       "4           0.630039           0.626110         0.630591        0.003903   \n",
       "5           0.732062           0.727259         0.727855        0.003220   \n",
       "6           0.733372           0.718527         0.726157        0.006068   \n",
       "7           0.710086           0.716490         0.713981        0.002792   \n",
       "\n",
       "   rank_test_score  \n",
       "0                4  \n",
       "1                3  \n",
       "2                2  \n",
       "3                1  \n",
       "4                8  \n",
       "5                5  \n",
       "6                6  \n",
       "7                7  "
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result2 = pd.DataFrame(grid2.cv_results_)\n",
    "pd.DataFrame(grid2.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a87bff3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_list = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "022be425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The index of 5 best combination:{7: 5, 4: 0, 3: 1, 2: 2, 1: 3, 5: 5}\n"
     ]
    }
   ],
   "source": [
    "best_list_2 = {}\n",
    "k = 1\n",
    "for i_iter,row_iter in enumerate(result2['rank_test_score']):\n",
    "    if row_iter >= 1 and row_iter <= 5:\n",
    "        best_list[row_iter] = i_iter\n",
    "    \n",
    "print(\"The index of 5 best combination:{}\".format(best_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "1ad86af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "rank_test_score:\n",
      "1\n",
      "____________________\n",
      "mean_test_score:\n",
      "0.7509941579866949\n",
      "____________________\n",
      "model parameters:\n",
      "{'clf': LogisticRegression(C=10, random_state=42), 'reduce_dim': NMF(init='random', n_components=500, random_state=42), 'vect': CountVectorizer(analyzer=<function doc_tokens_lemma at 0x7fbc68983430>,\n",
      "                min_df=3)}\n",
      "________________________________________________________________________________\n",
      "________________________________________________________________________________\n",
      "rank_test_score:\n",
      "2\n",
      "____________________\n",
      "mean_test_score:\n",
      "0.7489569406650086\n",
      "____________________\n",
      "model parameters:\n",
      "{'clf': LogisticRegression(C=10, random_state=42), 'reduce_dim': TruncatedSVD(n_components=500, random_state=42), 'vect': CountVectorizer(analyzer=<function doc_tokens_lemma at 0x7fbc68983430>,\n",
      "                min_df=3)}\n",
      "________________________________________________________________________________\n",
      "________________________________________________________________________________\n",
      "rank_test_score:\n",
      "3\n",
      "____________________\n",
      "mean_test_score:\n",
      "0.7478897518643789\n",
      "____________________\n",
      "model parameters:\n",
      "{'clf': SVC(C=10, kernel='linear', random_state=42), 'reduce_dim': NMF(init='random', n_components=500, random_state=42), 'vect': CountVectorizer(analyzer=<function doc_tokens_lemma at 0x7fbc68983430>,\n",
      "                min_df=3)}\n",
      "________________________________________________________________________________\n",
      "________________________________________________________________________________\n",
      "rank_test_score:\n",
      "4\n",
      "____________________\n",
      "mean_test_score:\n",
      "0.7463373617257371\n",
      "____________________\n",
      "model parameters:\n",
      "{'clf': SVC(C=10, kernel='linear', random_state=42), 'reduce_dim': TruncatedSVD(n_components=500, random_state=42), 'vect': CountVectorizer(analyzer=<function doc_tokens_lemma at 0x7fbc68983430>,\n",
      "                min_df=3)}\n",
      "________________________________________________________________________________\n",
      "________________________________________________________________________________\n",
      "rank_test_score:\n",
      "5\n",
      "____________________\n",
      "mean_test_score:\n",
      "0.7278550311305404\n",
      "____________________\n",
      "model parameters:\n",
      "{'clf': GaussianNB(), 'reduce_dim': NMF(init='random', n_components=500, random_state=42), 'vect': CountVectorizer(analyzer=<function doc_tokens_lemma at 0x7fbc68983430>,\n",
      "                min_df=3)}\n",
      "________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "while i <= 5:\n",
    "    index = best_list[i]\n",
    "    print(\"_\" * 80)\n",
    "    print(\"rank_test_score:\")\n",
    "    print(result2['rank_test_score'][index]) \n",
    "    print(\"_\" * 20)\n",
    "    print(\"mean_test_score:\")\n",
    "    print(result2['mean_test_score'][index])     \n",
    "    print(\"_\" * 20)\n",
    "    print(\"model parameters:\")\n",
    "    print(result2['params'][index])  \n",
    "    print(\"_\" * 80)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "7afe7340",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"results\",result2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
