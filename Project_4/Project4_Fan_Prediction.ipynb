{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f68d78e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from sklearn import metrics\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics.cluster import contingency_matrix, homogeneity_score, completeness_score, adjusted_rand_score, adjusted_mutual_info_score, v_measure_score\n",
    "from sklearn.metrics.cluster import contingency_matrix\n",
    "\n",
    "## Import libraries\n",
    "import re\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords' )\n",
    "\n",
    "import string\n",
    "from string import punctuation\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "716a90ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, json, datetime, pytz\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction import text\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from textblob import TextBlob\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from collections import Counter\n",
    "from collections import Counter\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2849c0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytz\n",
    "import datetime\n",
    "\n",
    "pst_tz = pytz.timezone('America/Los_Angeles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "695aaf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet(tweet):\n",
    "    '''\n",
    "    Utility function to clean the text in a tweet by removing \n",
    "    links and special characters using regex.\n",
    "    '''\n",
    "    return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?|\", \" \", tweet).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "72def57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(text, annotate=0):\n",
    "    with open(text+'.txt', 'r',encoding=\"utf8\") as f:\n",
    "        retweet_count, followers, place, title, time, fan = [],[],[],[],[],[]\n",
    "        sentiment_polarity, sentiment_score = [], []\n",
    "        pst_tz = pytz.timezone('US/Pacific') \n",
    "        for line in f:\n",
    "            tweet = json.loads(line)\n",
    "            time_now = datetime.datetime.fromtimestamp(tweet['citation_date'])\n",
    "            retweet_now = tweet['metrics']['citations']['total']\n",
    "            if tweet['tweet']['lang'] == 'en':\n",
    "                title.append(clean_tweet(tweet['title']).lower())\n",
    "                retweet_count.append(tweet['metrics']['citations']['total']) \n",
    "                followers.append(tweet['author']['followers'])\n",
    "                time.append(datetime.datetime.fromtimestamp(tweet['citation_date'], pst_tz))\n",
    "                place.append(tweet['tweet']['user']['location'])\n",
    "                fan.append(annotate)\n",
    "\n",
    "    d = {'title':title, 'retweet_count':retweet_count, 'followers':followers,\n",
    "         'time':time,'place':place, 'fan_base':fan}\n",
    "    df = pd.DataFrame(d)\n",
    "    return df   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "314f6840",
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = open('tweets_#gopatriots.txt', 'r')\n",
    "lines = file1.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7e7d6136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for line in lines:\n",
    "    json_object = json.loads(line)\n",
    "    if json_object['metrics']['citations']['total'] >= 10:\n",
    "        count = count + 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5d3f93df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_patriots = get_data('tweets_#gopatriots').append(get_data('tweets_#patriots'))\n",
    "df_patriots = get_data('tweets_#gopatriots')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "31c5e6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hawks = get_data('tweets_#gohawks', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "981a3cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12884\n",
      "142367\n"
     ]
    }
   ],
   "source": [
    "print(len(df_patriots))\n",
    "print(len(df_hawks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "281d5d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import libraries\n",
    "import re\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords' )\n",
    "\n",
    "import string\n",
    "from string import punctuation\n",
    "\n",
    "stop_words_skt = text.ENGLISH_STOP_WORDS\n",
    "stop_words_en = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d5e26ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_game = ['patriots', 'hawks', 'gopatriots', 'gohawks', 'patriot', 'hawk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1efac0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_stopwords = set.union(set(stop_words_en),set(punctuation),set(stop_words_skt), set(stop_words_game))\n",
    "wnl = nltk.wordnet.WordNetLemmatizer()\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "analyzer = CountVectorizer().build_analyzer()\n",
    "\n",
    "def penn2morphy(penntag):\n",
    "    \"\"\" \n",
    "    Converts Penn Treebank tags to WordNet. \n",
    "    \"\"\"\n",
    "    morphy_tag = {'NN':'n', 'JJ':'a',\n",
    "                  'VB':'v', 'RB':'r'}\n",
    "    try:\n",
    "        return morphy_tag[penntag[:2]]\n",
    "    except:\n",
    "        return 'n' \n",
    "\n",
    "def lemmatize_sent(list_word): \n",
    "    '''\n",
    "    Returns lemmatized set of tokens with pos tagging\n",
    "    '''\n",
    "    return [wnl.lemmatize(word.lower(), pos=penn2morphy(tag)) \n",
    "            for word, tag in pos_tag(list_word)]\n",
    "\n",
    "def stem_rmv_punc(doc): # this should have been at the sentence-level because the pos-tag performs best at sentence-level\n",
    "    return (word for word in lemmatize_sent(analyzer(doc)) if word not in combined_stopwords and not word.isdigit())\n",
    "\n",
    "def clean_tokens_lemma (tokens: list):\n",
    "    '''\n",
    "    Cleans set of tokens at sentence level by applying lemmatization at sentence level\n",
    "    '''\n",
    "    lower_txt = [token.lower() for token in tokens]\n",
    "    remove_words = [token for token in lower_txt if (not token.isdigit())\\\n",
    "                     and (token not in combined_stopwords) and (len(token)>1)]\n",
    "    lemmatize_tokens = lemmatize_sent(remove_words) \n",
    "    return lemmatize_tokens\n",
    "\n",
    "def doc_tokens_lemma (doc):\n",
    "    '''\n",
    "    Split the document at sentence level, clean it and return clean set of tokens at doc level\n",
    "    '''\n",
    "    doc = clean(doc)\n",
    "    list_sentences = sent_tokenize(doc)\n",
    "    doc_tokens = []\n",
    "    for sentence in list_sentences:\n",
    "        sentence = sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        tokens = clean_tokens_lemma(tokens)\n",
    "        doc_tokens.extend(tokens)\n",
    "#     print(doc_tokens)\n",
    "    return (word for word in doc_tokens)     \n",
    "\n",
    "def doc_tokens_lemma_woClean (doc):\n",
    "    '''\n",
    "    Perform lemmatization without any text cleaning\n",
    "    '''\n",
    "    list_sentences = sent_tokenize(doc)\n",
    "    doc_tokens = []\n",
    "    for sentence in list_sentences:\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        tokens = lemmatize_sent(tokens)\n",
    "        doc_tokens.extend(tokens)\n",
    "#     print(doc_tokens)\n",
    "    return (word for word in doc_tokens)   \n",
    "\n",
    "def doc_tokens_stem (doc):\n",
    "    '''\n",
    "    Clean full text using stemming. This function does not require split at sentence level.\n",
    "    '''\n",
    "    doc = clean(doc)\n",
    "    doc = doc.translate(str.maketrans('', '', string.punctuation))\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "    lower_txt = [token.lower() for token in tokens]\n",
    "    remove_words = [token for token in lower_txt if (not token.isdigit())\\\n",
    "                     and (token not in combined_stopwords) and (len(token)>1)]\n",
    "    stem_tokens = [stemmer.stem(token) for token in remove_words] \n",
    "    return (word for word in stem_tokens)\n",
    "\n",
    "def doc_tokens_stem_woClean (doc):\n",
    "    '''\n",
    "    Clean full text using stemming. This function does not require split at sentence level.\n",
    "    '''\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "    lower_txt = [token.lower() for token in tokens]\n",
    "    stem_tokens = [stemmer.stem(token) for token in lower_txt] \n",
    "    return (word for word in stem_tokens)\n",
    "    \n",
    "def doc_tokens (doc):\n",
    "    '''\n",
    "    Clean full text without any stemming or lemmatization\n",
    "    '''\n",
    "    doc = clean(doc)\n",
    "    doc = doc.translate(str.maketrans('', '', string.punctuation))\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "    lower_txt = [token.lower() for token in tokens]\n",
    "    remove_words = [token for token in lower_txt if (not token.isdigit())\\\n",
    "                     and (token not in combined_stopwords) and (len(token)>1)]\n",
    "    return (word for word in remove_words)\n",
    "    \n",
    "def doc_tokens_woClean (doc):\n",
    "    '''\n",
    "    Clean full text using stemming. This function does not require split at sentence level.\n",
    "    '''\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "    lower_txt = [token.lower() for token in tokens]\n",
    "    return (word for word in lower_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6752f1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    '''\n",
    "    Helps remove many HTML artefacts from the crawler's output.\n",
    "    '''\n",
    "    text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    texter = re.sub(r\"<br />\", \" \", text)\n",
    "    texter = re.sub(r\"&quot;\", \"\\\"\",texter)\n",
    "    texter = re.sub('&#39;', \"\\\"\", texter)\n",
    "    texter = re.sub('\\n', \" \", texter)\n",
    "    texter = re.sub(' u ',\" you \", texter)\n",
    "    texter = re.sub('`',\"\", texter)\n",
    "    texter = re.sub(' +', ' ', texter)\n",
    "    texter = re.sub(r\"(!)\\1+\", r\"!\", texter)\n",
    "    texter = re.sub(r\"(\\?)\\1+\", r\"?\", texter)\n",
    "    texter = re.sub('&amp;', 'and', texter)\n",
    "    texter = re.sub('\\r', ' ',texter)\n",
    "    clean = re.compile('<.*?>')\n",
    "    texter = texter.encode('ascii', 'ignore').decode('ascii')\n",
    "    texter = re.sub(clean, '', texter)\n",
    "    if texter == \"\":\n",
    "        texter = \"\"\n",
    "    return texter\n",
    "\n",
    "def clean_data(data):\n",
    "    for index,row in data.iterrows():\n",
    "        row['full_text'] = clean(row['full_text'])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3d0de5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def count_train_test(train,test):\n",
    "    \n",
    "    train_num = train.shape[0]\n",
    "    test_num = test.shape[0]\n",
    "    return train_num,test_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1dfbe024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train number:10307\n",
      "test number:2577\n"
     ]
    }
   ],
   "source": [
    "train_patriots, test_patriots = train_test_split(df_patriots, test_size=0.2, random_state = 42)\n",
    "\n",
    "train_num,test_num = count_train_test(train_patriots,test_patriots)\n",
    "print(\"train number:{}\".format(train_num))\n",
    "print(\"test number:{}\".format(test_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "485e8af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train number:113893\n",
      "test number:28474\n"
     ]
    }
   ],
   "source": [
    "train_hawks, test_hawks = train_test_split(df_hawks, test_size=0.2, random_state = 42)\n",
    "\n",
    "train_num,test_num = count_train_test(train_hawks, test_hawks)\n",
    "print(\"train number:{}\".format(train_num))\n",
    "print(\"test number:{}\".format(test_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3e9106ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF processed train data shape: (10307, 1915)\n",
      "TF-IDF processed test data shape: (2577, 1915)\n"
     ]
    }
   ],
   "source": [
    "## For Patriots\n",
    "\n",
    "count_vectorizer_patriots = CountVectorizer(min_df=3, stop_words='english', analyzer=doc_tokens_lemma, max_df=0.7)\n",
    "transformer = TfidfTransformer()\n",
    "\n",
    "X_train_counts_patriots = count_vectorizer_patriots.fit_transform(train_patriots['title'])\n",
    "X_train_tfidf_patriots = transformer.fit_transform(X_train_counts_patriots)\n",
    "\n",
    "X_test_counts_patriots = count_vectorizer_patriots.transform(test_patriots['title'])\n",
    "X_test_tfidf_patriots = transformer.transform(X_test_counts_patriots)\n",
    "\n",
    "print('TF-IDF processed train data shape:', X_train_tfidf_patriots.shape)\n",
    "print('TF-IDF processed test data shape:', X_test_tfidf_patriots.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fb2a0942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF processed train data shape: (113893, 10374)\n",
      "TF-IDF processed test data shape: (28474, 10374)\n"
     ]
    }
   ],
   "source": [
    "## For Hawks\n",
    "\n",
    "count_vectorizer_hawks = CountVectorizer(min_df=3, stop_words='english', analyzer=doc_tokens_lemma, max_df=0.7)\n",
    "transformer = TfidfTransformer()\n",
    "\n",
    "X_train_counts_hawks = count_vectorizer_hawks.fit_transform(train_hawks['title'])\n",
    "X_train_tfidf_hawks = transformer.fit_transform(X_train_counts_hawks)\n",
    "\n",
    "X_test_counts_hawks = count_vectorizer_hawks.transform(test_hawks['title'])\n",
    "X_test_tfidf_hawks = transformer.transform(X_test_counts_hawks)\n",
    "\n",
    "print('TF-IDF processed train data shape:', X_train_tfidf_hawks.shape)\n",
    "print('TF-IDF processed test data shape:', X_test_tfidf_hawks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "05a4580b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_patriots.append(train_hawks.sample(n=len(train_patriots)))\n",
    "test = test_patriots.append(test_hawks.sample(n=len(train_patriots)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5396d5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop_duplicates()\n",
    "test = test.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0fd6d1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from tempfile import mkdtemp\n",
    "from shutil import rmtree\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "11af0744",
   "metadata": {},
   "outputs": [],
   "source": [
    "cachedir = mkdtemp()\n",
    "memory = joblib.Memory(location=cachedir, verbose=10)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(min_df=3, analyzer=doc_tokens_lemma)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('reduce_dim', TruncatedSVD(random_state=42)),\n",
    "    ('clf', GaussianNB()),\n",
    "],\n",
    "memory=memory\n",
    ")\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        'vect': [CountVectorizer(min_df=3, analyzer=doc_tokens_lemma),\n",
    "        ],\n",
    "        'reduce_dim': [\n",
    "                        TruncatedSVD(n_components=500, random_state=42),\n",
    "#                         NMF(n_components=500, init='random', random_state=42),\n",
    "        ],\n",
    "        'clf': [\n",
    "                SVC(kernel='linear', C=10, random_state=42),\n",
    "                LogisticRegression(penalty='l2', C=10, random_state=42),\n",
    "                GaussianNB(),\n",
    "                RandomForestClassifier(max_depth=10, random_state=42)\n",
    "        ],\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ceb8cb9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "[Memory] Calling sklearn.pipeline._fit_transform_one...\n",
      "_fit_transform_one(CountVectorizer(analyzer=<function doc_tokens_lemma at 0x7fbc689839d0>,\n",
      "                min_df=3), \n",
      "7089      The Seahawks better not win this Super Bowl Go...\n",
      "8473      Yall seahawk fans quiet yall aint making no no...\n",
      "12112                    patriots GoPatriots Finish the Job\n",
      "12181                         SuperBowl reaction GoPatriots\n",
      "8794                      Don t let me down Pats gopatriots\n",
      "                                ...                        \n",
      "70978           How many yall going to the send off GoHawks\n",
      "119004             I hope the Seahawks beat Seattle GoHawks\n",
      "112548              GoHawks watching the from Venezuela Yay\n",
      "22201                    That 25 guy is pretty good GoHawks\n",
      "108966    If you do not let the world know who you re ch...\n",
      "Name: title, Length: 13742, dtype: object, \n",
      "7089      0\n",
      "8473      0\n",
      "12112     0\n",
      "12181     0\n",
      "8794      0\n",
      "         ..\n",
      "70978     1\n",
      "119004    1\n",
      "112548    1\n",
      "22201     1\n",
      "108966    1\n",
      "Name: fan_base, Length: 13742, dtype: int64, \n",
      "None, message_clsname='Pipeline', message=None)\n",
      "________________________________________________fit_transform_one - 5.3s, 0.1min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling sklearn.pipeline._fit_transform_one...\n",
      "_fit_transform_one(TfidfTransformer(), <13742x2594 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 76934 stored elements in Compressed Sparse Row format>, \n",
      "7089      0\n",
      "8473      0\n",
      "12112     0\n",
      "12181     0\n",
      "8794      0\n",
      "         ..\n",
      "70978     1\n",
      "119004    1\n",
      "112548    1\n",
      "22201     1\n",
      "108966    1\n",
      "Name: fan_base, Length: 13742, dtype: int64, \n",
      "None, message_clsname='Pipeline', message=None)\n",
      "________________________________________________fit_transform_one - 0.0s, 0.0min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling sklearn.pipeline._fit_transform_one...\n",
      "_fit_transform_one(TruncatedSVD(n_components=500, random_state=42), <13742x2594 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 76934 stored elements in Compressed Sparse Row format>, \n",
      "7089      0\n",
      "8473      0\n",
      "12112     0\n",
      "12181     0\n",
      "8794      0\n",
      "         ..\n",
      "70978     1\n",
      "119004    1\n",
      "112548    1\n",
      "22201     1\n",
      "108966    1\n",
      "Name: fan_base, Length: 13742, dtype: int64, \n",
      "None, message_clsname='Pipeline', message=None)\n",
      "________________________________________________fit_transform_one - 1.8s, 0.0min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling sklearn.pipeline._fit_transform_one...\n",
      "_fit_transform_one(CountVectorizer(analyzer=<function doc_tokens_lemma at 0x7fbc689839d0>,\n",
      "                min_df=3), \n",
      "6436      Lettttzzzz go Game time boom SuperBowl49 GoPat...\n",
      "1805      Is there a mercy rule in the NFL My god what a...\n",
      "2288      I don t like either team but for sake of compe...\n",
      "11736     Seahawks some sore losers bro tryna fight and ...\n",
      "7047      For the love of God please stop putting a Kard...\n",
      "                                ...                        \n",
      "70978           How many yall going to the send off GoHawks\n",
      "119004             I hope the Seahawks beat Seattle GoHawks\n",
      "112548              GoHawks watching the from Venezuela Yay\n",
      "22201                    That 25 guy is pretty good GoHawks\n",
      "108966    If you do not let the world know who you re ch...\n",
      "Name: title, Length: 13743, dtype: object, \n",
      "6436      0\n",
      "1805      0\n",
      "2288      0\n",
      "11736     0\n",
      "7047      0\n",
      "         ..\n",
      "70978     1\n",
      "119004    1\n",
      "112548    1\n",
      "22201     1\n",
      "108966    1\n",
      "Name: fan_base, Length: 13743, dtype: int64, \n",
      "None, message_clsname='Pipeline', message=None)\n",
      "________________________________________________fit_transform_one - 5.3s, 0.1min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling sklearn.pipeline._fit_transform_one...\n",
      "_fit_transform_one(TfidfTransformer(), <13743x2604 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 77067 stored elements in Compressed Sparse Row format>, \n",
      "6436      0\n",
      "1805      0\n",
      "2288      0\n",
      "11736     0\n",
      "7047      0\n",
      "         ..\n",
      "70978     1\n",
      "119004    1\n",
      "112548    1\n",
      "22201     1\n",
      "108966    1\n",
      "Name: fan_base, Length: 13743, dtype: int64, \n",
      "None, message_clsname='Pipeline', message=None)\n",
      "________________________________________________fit_transform_one - 0.0s, 0.0min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling sklearn.pipeline._fit_transform_one...\n",
      "_fit_transform_one(TruncatedSVD(n_components=500, random_state=42), <13743x2604 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 77067 stored elements in Compressed Sparse Row format>, \n",
      "6436      0\n",
      "1805      0\n",
      "2288      0\n",
      "11736     0\n",
      "7047      0\n",
      "         ..\n",
      "70978     1\n",
      "119004    1\n",
      "112548    1\n",
      "22201     1\n",
      "108966    1\n",
      "Name: fan_base, Length: 13743, dtype: int64, \n",
      "None, message_clsname='Pipeline', message=None)\n",
      "________________________________________________fit_transform_one - 1.7s, 0.0min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling sklearn.pipeline._fit_transform_one...\n",
      "_fit_transform_one(CountVectorizer(analyzer=<function doc_tokens_lemma at 0x7fbc689839d0>,\n",
      "                min_df=3), \n",
      "6436      Lettttzzzz go Game time boom SuperBowl49 GoPat...\n",
      "1805      Is there a mercy rule in the NFL My god what a...\n",
      "2288      I don t like either team but for sake of compe...\n",
      "11736     Seahawks some sore losers bro tryna fight and ...\n",
      "7047      For the love of God please stop putting a Kard...\n",
      "                                ...                        \n",
      "90344     We got that beer game going strong NFL GoHawks...\n",
      "42038     PACKERS ARE KEARSED SeahawksvsPackers Seahawks...\n",
      "101295             We have arrived 12thMan Seahawks GoHawks\n",
      "54444     What of country do you think will root for Sea...\n",
      "56028     Wilson s 4 picks were all intended for Kearse ...\n",
      "Name: title, Length: 13743, dtype: object, \n",
      "6436      0\n",
      "1805      0\n",
      "2288      0\n",
      "11736     0\n",
      "7047      0\n",
      "         ..\n",
      "90344     1\n",
      "42038     1\n",
      "101295    1\n",
      "54444     1\n",
      "56028     1\n",
      "Name: fan_base, Length: 13743, dtype: int64, \n",
      "None, message_clsname='Pipeline', message=None)\n",
      "________________________________________________fit_transform_one - 5.3s, 0.1min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling sklearn.pipeline._fit_transform_one...\n",
      "_fit_transform_one(TfidfTransformer(), <13743x2611 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 77247 stored elements in Compressed Sparse Row format>, \n",
      "6436      0\n",
      "1805      0\n",
      "2288      0\n",
      "11736     0\n",
      "7047      0\n",
      "         ..\n",
      "90344     1\n",
      "42038     1\n",
      "101295    1\n",
      "54444     1\n",
      "56028     1\n",
      "Name: fan_base, Length: 13743, dtype: int64, \n",
      "None, message_clsname='Pipeline', message=None)\n",
      "________________________________________________fit_transform_one - 0.0s, 0.0min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling sklearn.pipeline._fit_transform_one...\n",
      "_fit_transform_one(TruncatedSVD(n_components=500, random_state=42), <13743x2611 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 77247 stored elements in Compressed Sparse Row format>, \n",
      "6436      0\n",
      "1805      0\n",
      "2288      0\n",
      "11736     0\n",
      "7047      0\n",
      "         ..\n",
      "90344     1\n",
      "42038     1\n",
      "101295    1\n",
      "54444     1\n",
      "56028     1\n",
      "Name: fan_base, Length: 13743, dtype: int64, \n",
      "None, message_clsname='Pipeline', message=None)\n",
      "________________________________________________fit_transform_one - 1.8s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmpdlzmtq4_/joblib/sklearn/pipeline/_fit_transform_one/34158a0e020066669ba47f5a2e66f651\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmpdlzmtq4_/joblib/sklearn/pipeline/_fit_transform_one/1688a6574df6b34d692f889a011c4088\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmpdlzmtq4_/joblib/sklearn/pipeline/_fit_transform_one/a86da712acaa78d371c4fe74e5b4129a\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmpdlzmtq4_/joblib/sklearn/pipeline/_fit_transform_one/194d490c15417623e9e73a70faa8af1b\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.1s, 0.0min    : Loading _fit_transform_one from /tmp/tmpdlzmtq4_/joblib/sklearn/pipeline/_fit_transform_one/02f2e34dbc40f71c57daf7cae6d37661\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.1s, 0.0min    : Loading _fit_transform_one from /tmp/tmpdlzmtq4_/joblib/sklearn/pipeline/_fit_transform_one/d544a430f388a85d93e221cfa85a8fba\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmpdlzmtq4_/joblib/sklearn/pipeline/_fit_transform_one/0f73db2d45bf1af8d54e515a8a2c49e2\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.1s, 0.0min    : Loading _fit_transform_one from /tmp/tmpdlzmtq4_/joblib/sklearn/pipeline/_fit_transform_one/e1b77681a0b92237faa5a19d94462607\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.1s, 0.0min    : Loading _fit_transform_one from /tmp/tmpdlzmtq4_/joblib/sklearn/pipeline/_fit_transform_one/7557617569470e5adc437a6281ee2322\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmpdlzmtq4_/joblib/sklearn/pipeline/_fit_transform_one/34158a0e020066669ba47f5a2e66f651\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.1s, 0.0min    : Loading _fit_transform_one from /tmp/tmpdlzmtq4_/joblib/sklearn/pipeline/_fit_transform_one/1688a6574df6b34d692f889a011c4088\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.1s, 0.0min    : Loading _fit_transform_one from /tmp/tmpdlzmtq4_/joblib/sklearn/pipeline/_fit_transform_one/a86da712acaa78d371c4fe74e5b4129a\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmpdlzmtq4_/joblib/sklearn/pipeline/_fit_transform_one/194d490c15417623e9e73a70faa8af1b\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmpdlzmtq4_/joblib/sklearn/pipeline/_fit_transform_one/02f2e34dbc40f71c57daf7cae6d37661\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.1s, 0.0min    : Loading _fit_transform_one from /tmp/tmpdlzmtq4_/joblib/sklearn/pipeline/_fit_transform_one/d544a430f388a85d93e221cfa85a8fba\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmpdlzmtq4_/joblib/sklearn/pipeline/_fit_transform_one/0f73db2d45bf1af8d54e515a8a2c49e2\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmpdlzmtq4_/joblib/sklearn/pipeline/_fit_transform_one/e1b77681a0b92237faa5a19d94462607\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.1s, 0.0min    : Loading _fit_transform_one from /tmp/tmpdlzmtq4_/joblib/sklearn/pipeline/_fit_transform_one/7557617569470e5adc437a6281ee2322\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmpdlzmtq4_/joblib/sklearn/pipeline/_fit_transform_one/34158a0e020066669ba47f5a2e66f651\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmpdlzmtq4_/joblib/sklearn/pipeline/_fit_transform_one/1688a6574df6b34d692f889a011c4088\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmpdlzmtq4_/joblib/sklearn/pipeline/_fit_transform_one/a86da712acaa78d371c4fe74e5b4129a\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmpdlzmtq4_/joblib/sklearn/pipeline/_fit_transform_one/194d490c15417623e9e73a70faa8af1b\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmpdlzmtq4_/joblib/sklearn/pipeline/_fit_transform_one/02f2e34dbc40f71c57daf7cae6d37661\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.1s, 0.0min    : Loading _fit_transform_one from /tmp/tmpdlzmtq4_/joblib/sklearn/pipeline/_fit_transform_one/d544a430f388a85d93e221cfa85a8fba\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmpdlzmtq4_/joblib/sklearn/pipeline/_fit_transform_one/0f73db2d45bf1af8d54e515a8a2c49e2\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmpdlzmtq4_/joblib/sklearn/pipeline/_fit_transform_one/e1b77681a0b92237faa5a19d94462607\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.1s, 0.0min    : Loading _fit_transform_one from /tmp/tmpdlzmtq4_/joblib/sklearn/pipeline/_fit_transform_one/7557617569470e5adc437a6281ee2322\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling sklearn.pipeline._fit_transform_one...\n",
      "_fit_transform_one(CountVectorizer(analyzer=<function doc_tokens_lemma at 0x7fbc689839d0>,\n",
      "                min_df=3), \n",
      "6436      Lettttzzzz go Game time boom SuperBowl49 GoPat...\n",
      "1805      Is there a mercy rule in the NFL My god what a...\n",
      "2288      I don t like either team but for sake of compe...\n",
      "11736     Seahawks some sore losers bro tryna fight and ...\n",
      "7047      For the love of God please stop putting a Kard...\n",
      "                                ...                        \n",
      "70978           How many yall going to the send off GoHawks\n",
      "119004             I hope the Seahawks beat Seattle GoHawks\n",
      "112548              GoHawks watching the from Venezuela Yay\n",
      "22201                    That 25 guy is pretty good GoHawks\n",
      "108966    If you do not let the world know who you re ch...\n",
      "Name: title, Length: 20614, dtype: object, \n",
      "6436      0\n",
      "1805      0\n",
      "2288      0\n",
      "11736     0\n",
      "7047      0\n",
      "         ..\n",
      "70978     1\n",
      "119004    1\n",
      "112548    1\n",
      "22201     1\n",
      "108966    1\n",
      "Name: fan_base, Length: 20614, dtype: int64, \n",
      "None, message_clsname='Pipeline', message=None)\n",
      "________________________________________________fit_transform_one - 8.0s, 0.1min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling sklearn.pipeline._fit_transform_one...\n",
      "_fit_transform_one(TfidfTransformer(), <20614x3448 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 117704 stored elements in Compressed Sparse Row format>, \n",
      "6436      0\n",
      "1805      0\n",
      "2288      0\n",
      "11736     0\n",
      "7047      0\n",
      "         ..\n",
      "70978     1\n",
      "119004    1\n",
      "112548    1\n",
      "22201     1\n",
      "108966    1\n",
      "Name: fan_base, Length: 20614, dtype: int64, \n",
      "None, message_clsname='Pipeline', message=None)\n",
      "________________________________________________fit_transform_one - 0.0s, 0.0min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling sklearn.pipeline._fit_transform_one...\n",
      "_fit_transform_one(TruncatedSVD(n_components=500, random_state=42), <20614x3448 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 117704 stored elements in Compressed Sparse Row format>, \n",
      "6436      0\n",
      "1805      0\n",
      "2288      0\n",
      "11736     0\n",
      "7047      0\n",
      "         ..\n",
      "70978     1\n",
      "119004    1\n",
      "112548    1\n",
      "22201     1\n",
      "108966    1\n",
      "Name: fan_base, Length: 20614, dtype: int64, \n",
      "None, message_clsname='Pipeline', message=None)\n",
      "________________________________________________fit_transform_one - 2.7s, 0.0min\n"
     ]
    }
   ],
   "source": [
    "grid = GridSearchCV(pipeline, cv=3, n_jobs=1, param_grid=param_grid, scoring='accuracy')\n",
    "grid.fit(train.title, train.fan_base)\n",
    "rmtree(cachedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "019966fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_clf</th>\n",
       "      <th>param_reduce_dim</th>\n",
       "      <th>param_vect</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.308131</td>\n",
       "      <td>0.082243</td>\n",
       "      <td>2.752050</td>\n",
       "      <td>0.024338</td>\n",
       "      <td>SVC(C=10, kernel='linear', random_state=42)</td>\n",
       "      <td>TruncatedSVD(n_components=500, random_state=42)</td>\n",
       "      <td>CountVectorizer(analyzer=&lt;function doc_tokens_...</td>\n",
       "      <td>{'clf': SVC(C=10, kernel='linear', random_stat...</td>\n",
       "      <td>0.999563</td>\n",
       "      <td>0.999418</td>\n",
       "      <td>0.999418</td>\n",
       "      <td>0.999466</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.211244</td>\n",
       "      <td>0.005096</td>\n",
       "      <td>2.636499</td>\n",
       "      <td>0.009563</td>\n",
       "      <td>LogisticRegression(C=10, random_state=42)</td>\n",
       "      <td>TruncatedSVD(n_components=500, random_state=42)</td>\n",
       "      <td>CountVectorizer(analyzer=&lt;function doc_tokens_...</td>\n",
       "      <td>{'clf': LogisticRegression(C=10, random_state=...</td>\n",
       "      <td>0.999272</td>\n",
       "      <td>0.999127</td>\n",
       "      <td>0.999127</td>\n",
       "      <td>0.999175</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.188560</td>\n",
       "      <td>0.049809</td>\n",
       "      <td>2.665139</td>\n",
       "      <td>0.018704</td>\n",
       "      <td>GaussianNB()</td>\n",
       "      <td>TruncatedSVD(n_components=500, random_state=42)</td>\n",
       "      <td>CountVectorizer(analyzer=&lt;function doc_tokens_...</td>\n",
       "      <td>{'clf': GaussianNB(), 'reduce_dim': TruncatedS...</td>\n",
       "      <td>0.687864</td>\n",
       "      <td>0.684762</td>\n",
       "      <td>0.667152</td>\n",
       "      <td>0.679926</td>\n",
       "      <td>0.009121</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12.995217</td>\n",
       "      <td>0.083480</td>\n",
       "      <td>2.703887</td>\n",
       "      <td>0.012373</td>\n",
       "      <td>RandomForestClassifier(max_depth=10, random_st...</td>\n",
       "      <td>TruncatedSVD(n_components=500, random_state=42)</td>\n",
       "      <td>CountVectorizer(analyzer=&lt;function doc_tokens_...</td>\n",
       "      <td>{'clf': RandomForestClassifier(max_depth=10, r...</td>\n",
       "      <td>0.997526</td>\n",
       "      <td>0.997526</td>\n",
       "      <td>0.997089</td>\n",
       "      <td>0.997380</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0       8.308131      0.082243         2.752050        0.024338   \n",
       "1       0.211244      0.005096         2.636499        0.009563   \n",
       "2       0.188560      0.049809         2.665139        0.018704   \n",
       "3      12.995217      0.083480         2.703887        0.012373   \n",
       "\n",
       "                                           param_clf  \\\n",
       "0        SVC(C=10, kernel='linear', random_state=42)   \n",
       "1          LogisticRegression(C=10, random_state=42)   \n",
       "2                                       GaussianNB()   \n",
       "3  RandomForestClassifier(max_depth=10, random_st...   \n",
       "\n",
       "                                  param_reduce_dim  \\\n",
       "0  TruncatedSVD(n_components=500, random_state=42)   \n",
       "1  TruncatedSVD(n_components=500, random_state=42)   \n",
       "2  TruncatedSVD(n_components=500, random_state=42)   \n",
       "3  TruncatedSVD(n_components=500, random_state=42)   \n",
       "\n",
       "                                          param_vect  \\\n",
       "0  CountVectorizer(analyzer=<function doc_tokens_...   \n",
       "1  CountVectorizer(analyzer=<function doc_tokens_...   \n",
       "2  CountVectorizer(analyzer=<function doc_tokens_...   \n",
       "3  CountVectorizer(analyzer=<function doc_tokens_...   \n",
       "\n",
       "                                              params  split0_test_score  \\\n",
       "0  {'clf': SVC(C=10, kernel='linear', random_stat...           0.999563   \n",
       "1  {'clf': LogisticRegression(C=10, random_state=...           0.999272   \n",
       "2  {'clf': GaussianNB(), 'reduce_dim': TruncatedS...           0.687864   \n",
       "3  {'clf': RandomForestClassifier(max_depth=10, r...           0.997526   \n",
       "\n",
       "   split1_test_score  split2_test_score  mean_test_score  std_test_score  \\\n",
       "0           0.999418           0.999418         0.999466        0.000069   \n",
       "1           0.999127           0.999127         0.999175        0.000069   \n",
       "2           0.684762           0.667152         0.679926        0.009121   \n",
       "3           0.997526           0.997089         0.997380        0.000206   \n",
       "\n",
       "   rank_test_score  \n",
       "0                1  \n",
       "1                2  \n",
       "2                4  \n",
       "3                3  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = pd.DataFrame(grid.cv_results_)\n",
    "pd.DataFrame(grid.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3daeecc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The index of 5 best combination:{1: 0, 2: 1, 4: 2, 3: 3}\n"
     ]
    }
   ],
   "source": [
    "best_list = {}\n",
    "k = 1\n",
    "for i,row in enumerate(result['rank_test_score']):\n",
    "    if row >= 1 and row <= 5:\n",
    "        best_list[row] = i\n",
    "    \n",
    "print(\"The index of 5 best combination:{}\".format(best_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b5f4a770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "rank_test_score:\n",
      "1\n",
      "____________________\n",
      "mean_test_score:\n",
      "0.9994663773615744\n",
      "____________________\n",
      "model parameters:\n",
      "{'clf': SVC(C=10, kernel='linear', random_state=42), 'reduce_dim': TruncatedSVD(n_components=500, random_state=42), 'vect': CountVectorizer(analyzer=<function doc_tokens_lemma at 0x7fbc689839d0>,\n",
      "                min_df=3)}\n",
      "________________________________________________________________________________\n",
      "________________________________________________________________________________\n",
      "rank_test_score:\n",
      "2\n",
      "____________________\n",
      "mean_test_score:\n",
      "0.9991753130349887\n",
      "____________________\n",
      "model parameters:\n",
      "{'clf': LogisticRegression(C=10, random_state=42), 'reduce_dim': TruncatedSVD(n_components=500, random_state=42), 'vect': CountVectorizer(analyzer=<function doc_tokens_lemma at 0x7fbc689839d0>,\n",
      "                min_df=3)}\n",
      "________________________________________________________________________________\n",
      "________________________________________________________________________________\n",
      "rank_test_score:\n",
      "3\n",
      "____________________\n",
      "mean_test_score:\n",
      "0.9973804140012009\n",
      "____________________\n",
      "model parameters:\n",
      "{'clf': RandomForestClassifier(max_depth=10, random_state=42), 'reduce_dim': TruncatedSVD(n_components=500, random_state=42), 'vect': CountVectorizer(analyzer=<function doc_tokens_lemma at 0x7fbc689839d0>,\n",
      "                min_df=3)}\n",
      "________________________________________________________________________________\n",
      "________________________________________________________________________________\n",
      "rank_test_score:\n",
      "4\n",
      "____________________\n",
      "mean_test_score:\n",
      "0.6799258786302279\n",
      "____________________\n",
      "model parameters:\n",
      "{'clf': GaussianNB(), 'reduce_dim': TruncatedSVD(n_components=500, random_state=42), 'vect': CountVectorizer(analyzer=<function doc_tokens_lemma at 0x7fbc689839d0>,\n",
      "                min_df=3)}\n",
      "________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "while i <= 4:\n",
    "    index = best_list[i]\n",
    "    print(\"_\" * 80)\n",
    "    print(\"rank_test_score:\")\n",
    "    print(result['rank_test_score'][index]) \n",
    "    print(\"_\" * 20)\n",
    "    print(\"mean_test_score:\")\n",
    "    print(result['mean_test_score'][index])     \n",
    "    print(\"_\" * 20)\n",
    "    print(\"model parameters:\")\n",
    "    print(result['params'][index])  \n",
    "    print(\"_\" * 80)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "14735aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Try by removing keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "094ac45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cachedir = mkdtemp()\n",
    "memory = joblib.Memory(location=cachedir, verbose=10)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(min_df=3, analyzer=doc_tokens_lemma)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('reduce_dim', TruncatedSVD(random_state=42)),\n",
    "    ('clf', GaussianNB()),\n",
    "],\n",
    "memory=memory\n",
    ")\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        'vect': [CountVectorizer(min_df=3, analyzer=doc_tokens_lemma),\n",
    "        ],\n",
    "        'reduce_dim': [\n",
    "                        TruncatedSVD(n_components=500, random_state=42),\n",
    "                         NMF(n_components=500, init='random', random_state=42),\n",
    "        ],\n",
    "        'clf': [\n",
    "                SVC(kernel='linear', C=10, random_state=42),\n",
    "                 LogisticRegression(penalty='l2', C=10, random_state=42),\n",
    "                 GaussianNB(),\n",
    "                 RandomForestClassifier(max_depth=10, random_state=42)\n",
    "        ],\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "acf79960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "[Memory] Calling sklearn.pipeline._fit_transform_one...\n",
      "_fit_transform_one(CountVectorizer(analyzer=<function doc_tokens_lemma at 0x7fbc68983430>,\n",
      "                min_df=3), \n",
      "7089      The Seahawks better not win this Super Bowl Go...\n",
      "8473      Yall seahawk fans quiet yall aint making no no...\n",
      "12112                    patriots GoPatriots Finish the Job\n",
      "12181                         SuperBowl reaction GoPatriots\n",
      "8794                      Don t let me down Pats gopatriots\n",
      "                                ...                        \n",
      "70978           How many yall going to the send off GoHawks\n",
      "119004             I hope the Seahawks beat Seattle GoHawks\n",
      "112548              GoHawks watching the from Venezuela Yay\n",
      "22201                    That 25 guy is pretty good GoHawks\n",
      "108966    If you do not let the world know who you re ch...\n",
      "Name: title, Length: 13742, dtype: object, \n",
      "7089      0\n",
      "8473      0\n",
      "12112     0\n",
      "12181     0\n",
      "8794      0\n",
      "         ..\n",
      "70978     1\n",
      "119004    1\n",
      "112548    1\n",
      "22201     1\n",
      "108966    1\n",
      "Name: fan_base, Length: 13742, dtype: int64, \n",
      "None, message_clsname='Pipeline', message=None)\n",
      "________________________________________________fit_transform_one - 4.8s, 0.1min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling sklearn.pipeline._fit_transform_one...\n",
      "_fit_transform_one(TfidfTransformer(), <13742x2590 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 61744 stored elements in Compressed Sparse Row format>, \n",
      "7089      0\n",
      "8473      0\n",
      "12112     0\n",
      "12181     0\n",
      "8794      0\n",
      "         ..\n",
      "70978     1\n",
      "119004    1\n",
      "112548    1\n",
      "22201     1\n",
      "108966    1\n",
      "Name: fan_base, Length: 13742, dtype: int64, \n",
      "None, message_clsname='Pipeline', message=None)\n",
      "________________________________________________fit_transform_one - 0.0s, 0.0min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling sklearn.pipeline._fit_transform_one...\n",
      "_fit_transform_one(TruncatedSVD(n_components=500, random_state=42), <13742x2590 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 61744 stored elements in Compressed Sparse Row format>, \n",
      "7089      0\n",
      "8473      0\n",
      "12112     0\n",
      "12181     0\n",
      "8794      0\n",
      "         ..\n",
      "70978     1\n",
      "119004    1\n",
      "112548    1\n",
      "22201     1\n",
      "108966    1\n",
      "Name: fan_base, Length: 13742, dtype: int64, \n",
      "None, message_clsname='Pipeline', message=None)\n",
      "________________________________________________fit_transform_one - 1.8s, 0.0min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling sklearn.pipeline._fit_transform_one...\n",
      "_fit_transform_one(CountVectorizer(analyzer=<function doc_tokens_lemma at 0x7fbc68983430>,\n",
      "                min_df=3), \n",
      "6436      Lettttzzzz go Game time boom SuperBowl49 GoPat...\n",
      "1805      Is there a mercy rule in the NFL My god what a...\n",
      "2288      I don t like either team but for sake of compe...\n",
      "11736     Seahawks some sore losers bro tryna fight and ...\n",
      "7047      For the love of God please stop putting a Kard...\n",
      "                                ...                        \n",
      "70978           How many yall going to the send off GoHawks\n",
      "119004             I hope the Seahawks beat Seattle GoHawks\n",
      "112548              GoHawks watching the from Venezuela Yay\n",
      "22201                    That 25 guy is pretty good GoHawks\n",
      "108966    If you do not let the world know who you re ch...\n",
      "Name: title, Length: 13743, dtype: object, \n",
      "6436      0\n",
      "1805      0\n",
      "2288      0\n",
      "11736     0\n",
      "7047      0\n",
      "         ..\n",
      "70978     1\n",
      "119004    1\n",
      "112548    1\n",
      "22201     1\n",
      "108966    1\n",
      "Name: fan_base, Length: 13743, dtype: int64, \n",
      "None, message_clsname='Pipeline', message=None)\n",
      "________________________________________________fit_transform_one - 4.8s, 0.1min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling sklearn.pipeline._fit_transform_one...\n",
      "_fit_transform_one(TfidfTransformer(), <13743x2605 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 61920 stored elements in Compressed Sparse Row format>, \n",
      "6436      0\n",
      "1805      0\n",
      "2288      0\n",
      "11736     0\n",
      "7047      0\n",
      "         ..\n",
      "70978     1\n",
      "119004    1\n",
      "112548    1\n",
      "22201     1\n",
      "108966    1\n",
      "Name: fan_base, Length: 13743, dtype: int64, \n",
      "None, message_clsname='Pipeline', message=None)\n",
      "________________________________________________fit_transform_one - 0.0s, 0.0min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling sklearn.pipeline._fit_transform_one...\n",
      "_fit_transform_one(TruncatedSVD(n_components=500, random_state=42), <13743x2605 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 61920 stored elements in Compressed Sparse Row format>, \n",
      "6436      0\n",
      "1805      0\n",
      "2288      0\n",
      "11736     0\n",
      "7047      0\n",
      "         ..\n",
      "70978     1\n",
      "119004    1\n",
      "112548    1\n",
      "22201     1\n",
      "108966    1\n",
      "Name: fan_base, Length: 13743, dtype: int64, \n",
      "None, message_clsname='Pipeline', message=None)\n",
      "________________________________________________fit_transform_one - 1.7s, 0.0min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling sklearn.pipeline._fit_transform_one...\n",
      "_fit_transform_one(CountVectorizer(analyzer=<function doc_tokens_lemma at 0x7fbc68983430>,\n",
      "                min_df=3), \n",
      "6436      Lettttzzzz go Game time boom SuperBowl49 GoPat...\n",
      "1805      Is there a mercy rule in the NFL My god what a...\n",
      "2288      I don t like either team but for sake of compe...\n",
      "11736     Seahawks some sore losers bro tryna fight and ...\n",
      "7047      For the love of God please stop putting a Kard...\n",
      "                                ...                        \n",
      "90344     We got that beer game going strong NFL GoHawks...\n",
      "42038     PACKERS ARE KEARSED SeahawksvsPackers Seahawks...\n",
      "101295             We have arrived 12thMan Seahawks GoHawks\n",
      "54444     What of country do you think will root for Sea...\n",
      "56028     Wilson s 4 picks were all intended for Kearse ...\n",
      "Name: title, Length: 13743, dtype: object, \n",
      "6436      0\n",
      "1805      0\n",
      "2288      0\n",
      "11736     0\n",
      "7047      0\n",
      "         ..\n",
      "90344     1\n",
      "42038     1\n",
      "101295    1\n",
      "54444     1\n",
      "56028     1\n",
      "Name: fan_base, Length: 13743, dtype: int64, \n",
      "None, message_clsname='Pipeline', message=None)\n",
      "________________________________________________fit_transform_one - 4.7s, 0.1min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling sklearn.pipeline._fit_transform_one...\n",
      "_fit_transform_one(TfidfTransformer(), <13743x2612 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 62086 stored elements in Compressed Sparse Row format>, \n",
      "6436      0\n",
      "1805      0\n",
      "2288      0\n",
      "11736     0\n",
      "7047      0\n",
      "         ..\n",
      "90344     1\n",
      "42038     1\n",
      "101295    1\n",
      "54444     1\n",
      "56028     1\n",
      "Name: fan_base, Length: 13743, dtype: int64, \n",
      "None, message_clsname='Pipeline', message=None)\n",
      "________________________________________________fit_transform_one - 0.0s, 0.0min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling sklearn.pipeline._fit_transform_one...\n",
      "_fit_transform_one(TruncatedSVD(n_components=500, random_state=42), <13743x2612 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 62086 stored elements in Compressed Sparse Row format>, \n",
      "6436      0\n",
      "1805      0\n",
      "2288      0\n",
      "11736     0\n",
      "7047      0\n",
      "         ..\n",
      "90344     1\n",
      "42038     1\n",
      "101295    1\n",
      "54444     1\n",
      "56028     1\n",
      "Name: fan_base, Length: 13743, dtype: int64, \n",
      "None, message_clsname='Pipeline', message=None)\n",
      "________________________________________________fit_transform_one - 1.8s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/34158a0e020066669ba47f5a2e66f651\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/dccef523d46fd62e8e61b0bc98cc6dec\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling sklearn.pipeline._fit_transform_one...\n",
      "_fit_transform_one(NMF(init='random', n_components=500, random_state=42), <13742x2590 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 61744 stored elements in Compressed Sparse Row format>, \n",
      "7089      0\n",
      "8473      0\n",
      "12112     0\n",
      "12181     0\n",
      "8794      0\n",
      "         ..\n",
      "70978     1\n",
      "119004    1\n",
      "112548    1\n",
      "22201     1\n",
      "108966    1\n",
      "Name: fan_base, Length: 13742, dtype: int64, \n",
      "None, message_clsname='Pipeline', message=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______________________________________________fit_transform_one - 430.9s, 7.2min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/194d490c15417623e9e73a70faa8af1b\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/c6de3af9cca9fc6c1a0fc62812e5bebb\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling sklearn.pipeline._fit_transform_one...\n",
      "_fit_transform_one(NMF(init='random', n_components=500, random_state=42), <13743x2605 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 61920 stored elements in Compressed Sparse Row format>, \n",
      "6436      0\n",
      "1805      0\n",
      "2288      0\n",
      "11736     0\n",
      "7047      0\n",
      "         ..\n",
      "70978     1\n",
      "119004    1\n",
      "112548    1\n",
      "22201     1\n",
      "108966    1\n",
      "Name: fan_base, Length: 13743, dtype: int64, \n",
      "None, message_clsname='Pipeline', message=None)\n",
      "______________________________________________fit_transform_one - 541.5s, 9.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/0f73db2d45bf1af8d54e515a8a2c49e2\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/eb3dc91705478cc9f612e8142141f51a\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling sklearn.pipeline._fit_transform_one...\n",
      "_fit_transform_one(NMF(init='random', n_components=500, random_state=42), <13743x2612 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 62086 stored elements in Compressed Sparse Row format>, \n",
      "6436      0\n",
      "1805      0\n",
      "2288      0\n",
      "11736     0\n",
      "7047      0\n",
      "         ..\n",
      "90344     1\n",
      "42038     1\n",
      "101295    1\n",
      "54444     1\n",
      "56028     1\n",
      "Name: fan_base, Length: 13743, dtype: int64, \n",
      "None, message_clsname='Pipeline', message=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shivam/.local/lib/python3.8/site-packages/sklearn/decomposition/_nmf.py:1637: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____________________________________________fit_transform_one - 644.1s, 10.7min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/34158a0e020066669ba47f5a2e66f651\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/dccef523d46fd62e8e61b0bc98cc6dec\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/833d29a9d1f3df179332ede20b01b213\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/194d490c15417623e9e73a70faa8af1b\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.1s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/c6de3af9cca9fc6c1a0fc62812e5bebb\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.1s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/688d549e69dcfab0c10deec6013bcf75\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/0f73db2d45bf1af8d54e515a8a2c49e2\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.1s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/eb3dc91705478cc9f612e8142141f51a\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.1s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/ad0aaa286ebd0a1370c85be5dc1ed2ac\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/34158a0e020066669ba47f5a2e66f651\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.1s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/dccef523d46fd62e8e61b0bc98cc6dec\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.1s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/90f60c3d4f12d0b72de12eb97d5a3ccd\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shivam/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/194d490c15417623e9e73a70faa8af1b\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.1s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/c6de3af9cca9fc6c1a0fc62812e5bebb\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.1s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/0af5c04ffb38e90368e2bfdc3d6e7d6d\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shivam/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/0f73db2d45bf1af8d54e515a8a2c49e2\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.1s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/eb3dc91705478cc9f612e8142141f51a\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.1s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/35eb7cb7135cbc32991ace40dc148da6\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shivam/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/34158a0e020066669ba47f5a2e66f651\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.1s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/dccef523d46fd62e8e61b0bc98cc6dec\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.1s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/833d29a9d1f3df179332ede20b01b213\n",
      "___________________________________fit_transform_one cache loaded - 0.1s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/194d490c15417623e9e73a70faa8af1b\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.1s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/c6de3af9cca9fc6c1a0fc62812e5bebb\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.1s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/688d549e69dcfab0c10deec6013bcf75\n",
      "___________________________________fit_transform_one cache loaded - 0.1s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/0f73db2d45bf1af8d54e515a8a2c49e2\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/eb3dc91705478cc9f612e8142141f51a\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.1s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/ad0aaa286ebd0a1370c85be5dc1ed2ac\n",
      "___________________________________fit_transform_one cache loaded - 0.1s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/34158a0e020066669ba47f5a2e66f651\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/dccef523d46fd62e8e61b0bc98cc6dec\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/90f60c3d4f12d0b72de12eb97d5a3ccd\n",
      "___________________________________fit_transform_one cache loaded - 0.2s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/194d490c15417623e9e73a70faa8af1b\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/c6de3af9cca9fc6c1a0fc62812e5bebb\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.1s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/0af5c04ffb38e90368e2bfdc3d6e7d6d\n",
      "___________________________________fit_transform_one cache loaded - 0.2s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/0f73db2d45bf1af8d54e515a8a2c49e2\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/eb3dc91705478cc9f612e8142141f51a\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/35eb7cb7135cbc32991ace40dc148da6\n",
      "___________________________________fit_transform_one cache loaded - 0.2s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/34158a0e020066669ba47f5a2e66f651\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.1s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/dccef523d46fd62e8e61b0bc98cc6dec\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.1s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/833d29a9d1f3df179332ede20b01b213\n",
      "___________________________________fit_transform_one cache loaded - 0.2s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/194d490c15417623e9e73a70faa8af1b\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/c6de3af9cca9fc6c1a0fc62812e5bebb\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.1s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/688d549e69dcfab0c10deec6013bcf75\n",
      "___________________________________fit_transform_one cache loaded - 0.1s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/0f73db2d45bf1af8d54e515a8a2c49e2\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.1s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/eb3dc91705478cc9f612e8142141f51a\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.1s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/ad0aaa286ebd0a1370c85be5dc1ed2ac\n",
      "___________________________________fit_transform_one cache loaded - 0.1s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/34158a0e020066669ba47f5a2e66f651\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/dccef523d46fd62e8e61b0bc98cc6dec\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.1s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/90f60c3d4f12d0b72de12eb97d5a3ccd\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/194d490c15417623e9e73a70faa8af1b\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/c6de3af9cca9fc6c1a0fc62812e5bebb\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.1s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/0af5c04ffb38e90368e2bfdc3d6e7d6d\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/0f73db2d45bf1af8d54e515a8a2c49e2\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/eb3dc91705478cc9f612e8142141f51a\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n",
      "[Memory]0.0s, 0.0min    : Loading _fit_transform_one from /tmp/tmp99tunv2_/joblib/sklearn/pipeline/_fit_transform_one/35eb7cb7135cbc32991ace40dc148da6\n",
      "___________________________________fit_transform_one cache loaded - 0.0s, 0.0min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "[Memory] Calling sklearn.pipeline._fit_transform_one...\n",
      "_fit_transform_one(CountVectorizer(analyzer=<function doc_tokens_lemma at 0x7fbc68983430>,\n",
      "                min_df=3), \n",
      "6436      Lettttzzzz go Game time boom SuperBowl49 GoPat...\n",
      "1805      Is there a mercy rule in the NFL My god what a...\n",
      "2288      I don t like either team but for sake of compe...\n",
      "11736     Seahawks some sore losers bro tryna fight and ...\n",
      "7047      For the love of God please stop putting a Kard...\n",
      "                                ...                        \n",
      "70978           How many yall going to the send off GoHawks\n",
      "119004             I hope the Seahawks beat Seattle GoHawks\n",
      "112548              GoHawks watching the from Venezuela Yay\n",
      "22201                    That 25 guy is pretty good GoHawks\n",
      "108966    If you do not let the world know who you re ch...\n",
      "Name: title, Length: 20614, dtype: object, \n",
      "6436      0\n",
      "1805      0\n",
      "2288      0\n",
      "11736     0\n",
      "7047      0\n",
      "         ..\n",
      "70978     1\n",
      "119004    1\n",
      "112548    1\n",
      "22201     1\n",
      "108966    1\n",
      "Name: fan_base, Length: 20614, dtype: int64, \n",
      "None, message_clsname='Pipeline', message=None)\n",
      "________________________________________________fit_transform_one - 7.1s, 0.1min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling sklearn.pipeline._fit_transform_one...\n",
      "_fit_transform_one(TfidfTransformer(), <20614x3452 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 94968 stored elements in Compressed Sparse Row format>, \n",
      "6436      0\n",
      "1805      0\n",
      "2288      0\n",
      "11736     0\n",
      "7047      0\n",
      "         ..\n",
      "70978     1\n",
      "119004    1\n",
      "112548    1\n",
      "22201     1\n",
      "108966    1\n",
      "Name: fan_base, Length: 20614, dtype: int64, \n",
      "None, message_clsname='Pipeline', message=None)\n",
      "________________________________________________fit_transform_one - 0.0s, 0.0min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling sklearn.pipeline._fit_transform_one...\n",
      "_fit_transform_one(NMF(init='random', n_components=500, random_state=42), <20614x3452 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 94968 stored elements in Compressed Sparse Row format>, \n",
      "6436      0\n",
      "1805      0\n",
      "2288      0\n",
      "11736     0\n",
      "7047      0\n",
      "         ..\n",
      "70978     1\n",
      "119004    1\n",
      "112548    1\n",
      "22201     1\n",
      "108966    1\n",
      "Name: fan_base, Length: 20614, dtype: int64, \n",
      "None, message_clsname='Pipeline', message=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shivam/.local/lib/python3.8/site-packages/sklearn/decomposition/_nmf.py:1637: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____________________________________________fit_transform_one - 896.6s, 14.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shivam/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "grid2 = GridSearchCV(pipeline, cv=3, n_jobs=1, param_grid=param_grid, scoring='accuracy')\n",
    "grid2.fit(train.title, train.fan_base)\n",
    "rmtree(cachedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "4e63964f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_clf</th>\n",
       "      <th>param_reduce_dim</th>\n",
       "      <th>param_vect</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37.267668</td>\n",
       "      <td>0.251887</td>\n",
       "      <td>11.704723</td>\n",
       "      <td>0.099588</td>\n",
       "      <td>SVC(C=10, kernel='linear', random_state=42)</td>\n",
       "      <td>TruncatedSVD(n_components=500, random_state=42)</td>\n",
       "      <td>CountVectorizer(analyzer=&lt;function doc_tokens_...</td>\n",
       "      <td>{'clf': SVC(C=10, kernel='linear', random_stat...</td>\n",
       "      <td>0.747963</td>\n",
       "      <td>0.753020</td>\n",
       "      <td>0.738029</td>\n",
       "      <td>0.746337</td>\n",
       "      <td>0.006227</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570.100207</td>\n",
       "      <td>88.016414</td>\n",
       "      <td>77.853300</td>\n",
       "      <td>61.368086</td>\n",
       "      <td>SVC(C=10, kernel='linear', random_state=42)</td>\n",
       "      <td>NMF(init='random', n_components=500, random_st...</td>\n",
       "      <td>CountVectorizer(analyzer=&lt;function doc_tokens_...</td>\n",
       "      <td>{'clf': SVC(C=10, kernel='linear', random_stat...</td>\n",
       "      <td>0.748545</td>\n",
       "      <td>0.752001</td>\n",
       "      <td>0.743123</td>\n",
       "      <td>0.747890</td>\n",
       "      <td>0.003654</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.572136</td>\n",
       "      <td>0.119720</td>\n",
       "      <td>2.342068</td>\n",
       "      <td>0.006577</td>\n",
       "      <td>LogisticRegression(C=10, random_state=42)</td>\n",
       "      <td>TruncatedSVD(n_components=500, random_state=42)</td>\n",
       "      <td>CountVectorizer(analyzer=&lt;function doc_tokens_...</td>\n",
       "      <td>{'clf': LogisticRegression(C=10, random_state=...</td>\n",
       "      <td>0.750582</td>\n",
       "      <td>0.753165</td>\n",
       "      <td>0.743123</td>\n",
       "      <td>0.748957</td>\n",
       "      <td>0.004258</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.661059</td>\n",
       "      <td>0.044667</td>\n",
       "      <td>68.292213</td>\n",
       "      <td>60.461573</td>\n",
       "      <td>LogisticRegression(C=10, random_state=42)</td>\n",
       "      <td>NMF(init='random', n_components=500, random_st...</td>\n",
       "      <td>CountVectorizer(analyzer=&lt;function doc_tokens_...</td>\n",
       "      <td>{'clf': LogisticRegression(C=10, random_state=...</td>\n",
       "      <td>0.757421</td>\n",
       "      <td>0.750255</td>\n",
       "      <td>0.745306</td>\n",
       "      <td>0.750994</td>\n",
       "      <td>0.004974</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.316643</td>\n",
       "      <td>0.034054</td>\n",
       "      <td>2.340587</td>\n",
       "      <td>0.007816</td>\n",
       "      <td>GaussianNB()</td>\n",
       "      <td>TruncatedSVD(n_components=500, random_state=42)</td>\n",
       "      <td>CountVectorizer(analyzer=&lt;function doc_tokens_...</td>\n",
       "      <td>{'clf': GaussianNB(), 'reduce_dim': TruncatedS...</td>\n",
       "      <td>0.635623</td>\n",
       "      <td>0.630039</td>\n",
       "      <td>0.626110</td>\n",
       "      <td>0.630591</td>\n",
       "      <td>0.003903</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.403428</td>\n",
       "      <td>0.053601</td>\n",
       "      <td>67.056438</td>\n",
       "      <td>60.749883</td>\n",
       "      <td>GaussianNB()</td>\n",
       "      <td>NMF(init='random', n_components=500, random_st...</td>\n",
       "      <td>CountVectorizer(analyzer=&lt;function doc_tokens_...</td>\n",
       "      <td>{'clf': GaussianNB(), 'reduce_dim': NMF(init='...</td>\n",
       "      <td>0.724243</td>\n",
       "      <td>0.732062</td>\n",
       "      <td>0.727259</td>\n",
       "      <td>0.727855</td>\n",
       "      <td>0.003220</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>14.516446</td>\n",
       "      <td>0.075953</td>\n",
       "      <td>2.375185</td>\n",
       "      <td>0.007645</td>\n",
       "      <td>RandomForestClassifier(max_depth=10, random_st...</td>\n",
       "      <td>TruncatedSVD(n_components=500, random_state=42)</td>\n",
       "      <td>CountVectorizer(analyzer=&lt;function doc_tokens_...</td>\n",
       "      <td>{'clf': RandomForestClassifier(max_depth=10, r...</td>\n",
       "      <td>0.726572</td>\n",
       "      <td>0.733372</td>\n",
       "      <td>0.718527</td>\n",
       "      <td>0.726157</td>\n",
       "      <td>0.006068</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.566802</td>\n",
       "      <td>0.025442</td>\n",
       "      <td>67.548493</td>\n",
       "      <td>60.777820</td>\n",
       "      <td>RandomForestClassifier(max_depth=10, random_st...</td>\n",
       "      <td>NMF(init='random', n_components=500, random_st...</td>\n",
       "      <td>CountVectorizer(analyzer=&lt;function doc_tokens_...</td>\n",
       "      <td>{'clf': RandomForestClassifier(max_depth=10, r...</td>\n",
       "      <td>0.715367</td>\n",
       "      <td>0.710086</td>\n",
       "      <td>0.716490</td>\n",
       "      <td>0.713981</td>\n",
       "      <td>0.002792</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0      37.267668      0.251887        11.704723        0.099588   \n",
       "1     570.100207     88.016414        77.853300       61.368086   \n",
       "2       0.572136      0.119720         2.342068        0.006577   \n",
       "3       0.661059      0.044667        68.292213       60.461573   \n",
       "4       0.316643      0.034054         2.340587        0.007816   \n",
       "5       0.403428      0.053601        67.056438       60.749883   \n",
       "6      14.516446      0.075953         2.375185        0.007645   \n",
       "7       1.566802      0.025442        67.548493       60.777820   \n",
       "\n",
       "                                           param_clf  \\\n",
       "0        SVC(C=10, kernel='linear', random_state=42)   \n",
       "1        SVC(C=10, kernel='linear', random_state=42)   \n",
       "2          LogisticRegression(C=10, random_state=42)   \n",
       "3          LogisticRegression(C=10, random_state=42)   \n",
       "4                                       GaussianNB()   \n",
       "5                                       GaussianNB()   \n",
       "6  RandomForestClassifier(max_depth=10, random_st...   \n",
       "7  RandomForestClassifier(max_depth=10, random_st...   \n",
       "\n",
       "                                    param_reduce_dim  \\\n",
       "0    TruncatedSVD(n_components=500, random_state=42)   \n",
       "1  NMF(init='random', n_components=500, random_st...   \n",
       "2    TruncatedSVD(n_components=500, random_state=42)   \n",
       "3  NMF(init='random', n_components=500, random_st...   \n",
       "4    TruncatedSVD(n_components=500, random_state=42)   \n",
       "5  NMF(init='random', n_components=500, random_st...   \n",
       "6    TruncatedSVD(n_components=500, random_state=42)   \n",
       "7  NMF(init='random', n_components=500, random_st...   \n",
       "\n",
       "                                          param_vect  \\\n",
       "0  CountVectorizer(analyzer=<function doc_tokens_...   \n",
       "1  CountVectorizer(analyzer=<function doc_tokens_...   \n",
       "2  CountVectorizer(analyzer=<function doc_tokens_...   \n",
       "3  CountVectorizer(analyzer=<function doc_tokens_...   \n",
       "4  CountVectorizer(analyzer=<function doc_tokens_...   \n",
       "5  CountVectorizer(analyzer=<function doc_tokens_...   \n",
       "6  CountVectorizer(analyzer=<function doc_tokens_...   \n",
       "7  CountVectorizer(analyzer=<function doc_tokens_...   \n",
       "\n",
       "                                              params  split0_test_score  \\\n",
       "0  {'clf': SVC(C=10, kernel='linear', random_stat...           0.747963   \n",
       "1  {'clf': SVC(C=10, kernel='linear', random_stat...           0.748545   \n",
       "2  {'clf': LogisticRegression(C=10, random_state=...           0.750582   \n",
       "3  {'clf': LogisticRegression(C=10, random_state=...           0.757421   \n",
       "4  {'clf': GaussianNB(), 'reduce_dim': TruncatedS...           0.635623   \n",
       "5  {'clf': GaussianNB(), 'reduce_dim': NMF(init='...           0.724243   \n",
       "6  {'clf': RandomForestClassifier(max_depth=10, r...           0.726572   \n",
       "7  {'clf': RandomForestClassifier(max_depth=10, r...           0.715367   \n",
       "\n",
       "   split1_test_score  split2_test_score  mean_test_score  std_test_score  \\\n",
       "0           0.753020           0.738029         0.746337        0.006227   \n",
       "1           0.752001           0.743123         0.747890        0.003654   \n",
       "2           0.753165           0.743123         0.748957        0.004258   \n",
       "3           0.750255           0.745306         0.750994        0.004974   \n",
       "4           0.630039           0.626110         0.630591        0.003903   \n",
       "5           0.732062           0.727259         0.727855        0.003220   \n",
       "6           0.733372           0.718527         0.726157        0.006068   \n",
       "7           0.710086           0.716490         0.713981        0.002792   \n",
       "\n",
       "   rank_test_score  \n",
       "0                4  \n",
       "1                3  \n",
       "2                2  \n",
       "3                1  \n",
       "4                8  \n",
       "5                5  \n",
       "6                6  \n",
       "7                7  "
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result2 = pd.DataFrame(grid2.cv_results_)\n",
    "pd.DataFrame(grid2.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "4205b065",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_list = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "23bb01d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The index of 5 best combination:{7: 5, 4: 0, 3: 1, 2: 2, 1: 3, 5: 5}\n"
     ]
    }
   ],
   "source": [
    "best_list_2 = {}\n",
    "k = 1\n",
    "for i_iter,row_iter in enumerate(result2['rank_test_score']):\n",
    "    if row_iter >= 1 and row_iter <= 5:\n",
    "        best_list[row_iter] = i_iter\n",
    "    \n",
    "print(\"The index of 5 best combination:{}\".format(best_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "2c309109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "rank_test_score:\n",
      "1\n",
      "____________________\n",
      "mean_test_score:\n",
      "0.7509941579866949\n",
      "____________________\n",
      "model parameters:\n",
      "{'clf': LogisticRegression(C=10, random_state=42), 'reduce_dim': NMF(init='random', n_components=500, random_state=42), 'vect': CountVectorizer(analyzer=<function doc_tokens_lemma at 0x7fbc68983430>,\n",
      "                min_df=3)}\n",
      "________________________________________________________________________________\n",
      "________________________________________________________________________________\n",
      "rank_test_score:\n",
      "2\n",
      "____________________\n",
      "mean_test_score:\n",
      "0.7489569406650086\n",
      "____________________\n",
      "model parameters:\n",
      "{'clf': LogisticRegression(C=10, random_state=42), 'reduce_dim': TruncatedSVD(n_components=500, random_state=42), 'vect': CountVectorizer(analyzer=<function doc_tokens_lemma at 0x7fbc68983430>,\n",
      "                min_df=3)}\n",
      "________________________________________________________________________________\n",
      "________________________________________________________________________________\n",
      "rank_test_score:\n",
      "3\n",
      "____________________\n",
      "mean_test_score:\n",
      "0.7478897518643789\n",
      "____________________\n",
      "model parameters:\n",
      "{'clf': SVC(C=10, kernel='linear', random_state=42), 'reduce_dim': NMF(init='random', n_components=500, random_state=42), 'vect': CountVectorizer(analyzer=<function doc_tokens_lemma at 0x7fbc68983430>,\n",
      "                min_df=3)}\n",
      "________________________________________________________________________________\n",
      "________________________________________________________________________________\n",
      "rank_test_score:\n",
      "4\n",
      "____________________\n",
      "mean_test_score:\n",
      "0.7463373617257371\n",
      "____________________\n",
      "model parameters:\n",
      "{'clf': SVC(C=10, kernel='linear', random_state=42), 'reduce_dim': TruncatedSVD(n_components=500, random_state=42), 'vect': CountVectorizer(analyzer=<function doc_tokens_lemma at 0x7fbc68983430>,\n",
      "                min_df=3)}\n",
      "________________________________________________________________________________\n",
      "________________________________________________________________________________\n",
      "rank_test_score:\n",
      "5\n",
      "____________________\n",
      "mean_test_score:\n",
      "0.7278550311305404\n",
      "____________________\n",
      "model parameters:\n",
      "{'clf': GaussianNB(), 'reduce_dim': NMF(init='random', n_components=500, random_state=42), 'vect': CountVectorizer(analyzer=<function doc_tokens_lemma at 0x7fbc68983430>,\n",
      "                min_df=3)}\n",
      "________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "while i <= 5:\n",
    "    index = best_list[i]\n",
    "    print(\"_\" * 80)\n",
    "    print(\"rank_test_score:\")\n",
    "    print(result2['rank_test_score'][index]) \n",
    "    print(\"_\" * 20)\n",
    "    print(\"mean_test_score:\")\n",
    "    print(result2['mean_test_score'][index])     \n",
    "    print(\"_\" * 20)\n",
    "    print(\"model parameters:\")\n",
    "    print(result2['params'][index])  \n",
    "    print(\"_\" * 80)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "867ccdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"results\",result2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
